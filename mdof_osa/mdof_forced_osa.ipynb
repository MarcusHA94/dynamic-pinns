{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDOF PINN - n-step ahead prediction\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "The example problem we solve here is the 3DOF nonlinear-stiffness oscillator defined in state space:\n",
    "$$\n",
    "\\dot{\\mathbf{z}} = \\mathbf{A}\\mathbf{z} + \\mathbf{A}_n\\mathbf{z}_n + \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "\\mathbf{z} = \\left\\{ x_1, x_2, ... , x_n, \\dot{x}_1, \\dot{x}_2, ... , \\dot{x}_n \\right\\}^T, \\quad\n",
    "\\mathbf{f} = \\left\\{ f_1, f_2, ... , f_n \\right\\}^T\n",
    "$$\n",
    "and $\\mathbf{z}_n$ is the nonlinear state vector.\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 0 & \\mathbf{I} \\\\ -\\mathbf{M}^{-1}\\mathbf{K} & -\\mathbf{M}^{-1}\\mathbf{C} \\end{bmatrix}, \\quad\n",
    "\\mathbf{A}_n = \\begin{bmatrix} 0 \\\\ -\\mathbf{M}^{-1} \\mathbf{K}_n \\end{bmatrix}, \\quad\n",
    "\\mathbf{H} = \\begin{bmatrix} 0 \\\\ \\mathbf{M}^{-1} \\end{bmatrix}\n",
    "$$\n",
    "with the initial conditions\n",
    "$$\n",
    "\\mathbf{x}(0) = \\mathbf{x}_0~~,~~\\dot{\\mathbf{x}}(0) = \\dot{\\mathbf{x}}_0~~,~~\\mathbf{z}_0 = \\{\\mathbf{x}_0,\\dot{\\mathbf{x}}_0\\}^T\n",
    "$$\n",
    "\n",
    "As an example, for a 3DOF system with cubic nonlinearities, fixed at the first degree of freedom:\n",
    "$$\n",
    "\\mathbf{z}_n = g_n(\\mathbf{z}) = \\left\\{ x_1^3, (x_2-x_1)^3, (x_3-x_2)^3 \\right\\}^T, \\quad\n",
    "\\mathbf{K}_n = \\begin{bmatrix} k_{n,1} & -k_{n,2} & 0 \\\\ 0 & k_{n,2} & -k_{n,3} \\\\ 0 & 0 & k_{n,3} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The aim is to predict the state at some time horizon ahead, using the previous state and force;\n",
    "$$\n",
    "\\mathbf{z}^{(n+1)} = \\mathcal{F}(\\mathbf{z}^{(n)}, t, f)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mdof_osa_pinn import osa_pinn_mdof, normalise, ParamClipper\n",
    "import dynasim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.stats import qmc\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 512\n",
    "time = np.linspace(0,60,nt)\n",
    "\n",
    "F0 = 1.0  # N\n",
    "n_dof = 3\n",
    "\n",
    "# set physical parameters\n",
    "k_ = np.array([15.0, 30.0, 20.0])\n",
    "c_ = np.array([0.25, 0.3, 0.2])\n",
    "m_ = np.array([10.0, 10.0, 10.0])\n",
    "kn_ = np.array([100.0, 0.0, 0.0])\n",
    "\n",
    "# create nonlinearity\n",
    "cubic_nonlin = dynasim.nonlinearities.exponent_stiffness(kn_, exponent=3, dofs=n_dof)\n",
    "\n",
    "# instantiate system\n",
    "system = dynasim.systems.cantilever(m_, c_, k_, dofs=n_dof, nonlinearity=cubic_nonlin)\n",
    "\n",
    "true_params = {\n",
    "    'm_' : system.m_,\n",
    "    'c_' : system.c_,\n",
    "    'k_' : system.k_,\n",
    "    'kn_' : kn_\n",
    "}\n",
    "\n",
    "# generate excitations\n",
    "system.excitations = [\n",
    "    dynasim.actuators.sinusoid(2.1, f0=F0, phi=0.0),\n",
    "    dynasim.actuators.sinusoid(0.8, f0=F0, phi=1.0),\n",
    "    None]\n",
    "\n",
    "x0 = np.array([-2.0, 0.0, 0.0])\n",
    "v0 = np.array([1.0, 0.0, 0.0])\n",
    "z0 = np.concatenate((x0, v0), axis=0)\n",
    "\n",
    "data = system.simulate(time, z0=None)\n",
    "\n",
    "xx, vv = torch.tensor(data['x'], dtype=torch.float32).T, torch.tensor(data['xdot'], dtype=torch.float32).T\n",
    "F = system.f\n",
    "time = torch.tensor(time).view(-1,1).to(torch.float32)\n",
    "F = torch.tensor(F.T).to(torch.float32)\n",
    "\n",
    "# slice out a number of points\n",
    "sampler = qmc.Sobol(d=1, seed=43810)\n",
    "sub_ind = np.sort(sampler.integers(l_bounds=nt, n=int(nt/4)), axis=0).squeeze()\n",
    "# sub_ind = np.arange(0, int(nt/1), 4)\n",
    "\n",
    "t_data = time[sub_ind]\n",
    "x_data = xx[sub_ind,:]\n",
    "v_data = vv[sub_ind,:]\n",
    "F_data = F[sub_ind,:]\n",
    "\n",
    "if n_dof > 4:\n",
    "    sub_rows = n_dof // 4 + int((n_dof%4)!=0)\n",
    "    sub_cols = 4\n",
    "else:\n",
    "    sub_rows = 1\n",
    "    sub_cols = n_dof\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(4*sub_cols,8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(time, xx[:,p_count], color=\"tab:blue\", label=\"Displacement\")\n",
    "        axs[j*3,i].grid()\n",
    "        axs[j*3,i].scatter(t_data, x_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3,i].legend()\n",
    "\n",
    "        axs[j*3+1,i].plot(time, vv[:,p_count], color=\"tab:red\", label=\"Velocity\")\n",
    "        axs[j*3+1,i].grid()\n",
    "        axs[j*3+1,i].scatter(t_data, v_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+1,i].legend()\n",
    "\n",
    "        axs[j*3+2,i].plot(time, F[:,p_count], color=\"tab:gray\", label=\"Forcing\")\n",
    "        # axs[j*3+2,i].scatter(t_data, F_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+2,i].legend()\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise and create some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hat, alpha_t = normalise(time, \"range\")\n",
    "x_hat_gt, alpha_x = normalise(xx, \"range\", \"all\")\n",
    "v_hat_gt, alpha_v = normalise(vv, \"range\", \"all\")\n",
    "F_hat_gt, alpha_F = normalise(F, \"range\", \"all\")\n",
    "\n",
    "t_data = t_hat[sub_ind]\n",
    "x_data = x_hat_gt[sub_ind,:]\n",
    "v_data = v_hat_gt[sub_ind,:]\n",
    "F_data = F_hat_gt[sub_ind,:]\n",
    "\n",
    "T_hat = t_data[1].item()-t_data[0].item()\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(4*sub_cols,8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(t_hat, x_hat_gt[:,i], color=\"tab:blue\", label=\"Displacement\")\n",
    "        # axs[j*3,i].scatter(t_data, x_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3,i].grid()\n",
    "        axs[j*3,i].legend()\n",
    "\n",
    "        axs[j*3+1,i].plot(t_hat, v_hat_gt[:,i], color=\"tab:red\", label=\"Velocity\")\n",
    "        # axs[j*3+1,i].scatter(t_data, v_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+1,i].grid()\n",
    "        axs[j*3+1,i].legend()\n",
    "\n",
    "        axs[j*3+2,i].plot(t_hat, F_hat_gt[:,i], color=\"tab:gray\", label=\"Forcing\")\n",
    "        # axs[j*3+2,i].scatter(t_data, F_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+2,i].legend()\n",
    "\n",
    "        p_count += 1\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_uppercase)\n",
    "mosaic_key = ''\n",
    "alph_count = 0\n",
    "for j in range(sub_rows):\n",
    "    mosaic_key += ''.join(alphabet[alph_count:alph_count+sub_cols]) + ';' + ''.join(alphabet[alph_count+sub_cols:alph_count+2*sub_cols]) + ';'\n",
    "    alph_count += 2*sub_cols\n",
    "mosaic_key += ''.join([alphabet[alph_count]]*sub_cols)\n",
    "print(mosaic_key)\n",
    "\n",
    "def plot_joint_loss_hist(ax,loss_hist):\n",
    "    n_epoch = len(loss_hist)\n",
    "    labels = [\"L_obs\", \"L_ic\", \"L_cc\", \"L_ode\", \"L\"]\n",
    "    colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"tab:cyan\", \"black\"]\n",
    "    # labels = [\"L_obs\", \"L_ic\", \"L_ode\", \"L_ed_b\", \"L_ed_p\", \"L\"]\n",
    "    # colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"black\"]\n",
    "    ax.cla()\n",
    "    for i in range(len(labels)):\n",
    "        ax.plot(np.arange(1,n_epoch+1),loss_hist[:,i],color=colors[i],label=labels[i])\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_result(axs_m, ground_truth, data, prediction, alphas, R_=None, axs2=None):\n",
    "    for ax in axs_m:\n",
    "        axs_m[ax].cla()\n",
    "        if axs2 is not None:\n",
    "            axs2[ax].cla()\n",
    "    # axs = np.array([[axs_m[alphabet[i]] for i in range(n_dof)],[axs_m[alphabet[n_dof+i]] for i in range(n_dof)]])\n",
    "    axs_top_list = []\n",
    "    for j in range(sub_rows):\n",
    "        axs_top_list.append([axs_m[alphabet[2*sub_cols*j+i]] for i in range(sub_cols)])\n",
    "        axs_top_list.append([axs_m[alphabet[2*sub_cols*j+sub_cols+i]] for i in range(sub_cols)])\n",
    "    axs_top = np.array(axs_top_list)\n",
    "\n",
    "    if axs2 is not None:\n",
    "        axs2_top_list = []\n",
    "        for j in range(sub_rows):\n",
    "            axs2_top_list.append([axs2[alphabet[2*sub_cols*j+i]] for i in range(sub_cols)])\n",
    "            axs2_top_list.append([axs2[alphabet[2*sub_cols*j+sub_cols+i]] for i in range(sub_cols)])\n",
    "        axs2_top = np.array(axs2_top_list)\n",
    "\n",
    "    plot_keys = [\"x_hat\", \"v_hat\"]\n",
    "    plot_cols = [\"tab:blue\", \"tab:red\"]\n",
    "    p_count = 0\n",
    "    for j in range(sub_rows):\n",
    "        for i in range(sub_cols):\n",
    "            for n in range(2):\n",
    "                axs_top[j*2+n,i].plot(data[\"t_hat\"].detach()*alphas[\"t\"], data[plot_keys[n]][:,p_count].detach()*alphas[\"x\"], color=\"tab:olive\", linewidth=1, alpha=0.8, label='Training data')\n",
    "                axs_top[j*2+n,i].plot(ground_truth[\"t_hat\"]*alphas[\"t\"], ground_truth[plot_keys[n]][:,p_count]*alphas[\"x\"], color=\"grey\", linewidth=2, alpha=0.5, label=\"Exact solution\")\n",
    "                axs_top[j*2+n,i].plot(prediction[\"t_hat\"].detach()*alphas[\"t\"], prediction[plot_keys[n]][:,p_count]*alphas[\"x\"], color=plot_cols[n], linewidth=2, alpha=0.8, linestyle='--', label=\"Neural network prediction\")\n",
    "                # axs_top[j*2+n,i].scatter(data[\"t_hat\"].detach()*alphas[\"t\"], data[plot_keys[n]][:,p_count].detach()*alphas[\"x\"], s=30, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "                # l = axs[i,j].legend(frameon=False, fontsize=\"large\")\n",
    "                xL = torch.amax(ground_truth[\"t_hat\"])*alphas[\"t\"]\n",
    "                yL = torch.amax(torch.abs(ground_truth[plot_keys[n]][:,p_count]))*alphas[\"x\"]\n",
    "                axs_top[j*2+n,i].set_xlim(-0.05*xL, 1.05*xL)\n",
    "                axs_top[j*2+n,i].set_ylim(-1.1*yL, 1.1*yL)\n",
    "            if R_ is not None:\n",
    "                axs2_top[j*2,i].plot(prediction[\"t_hat\"].detach()*alphas[\"t\"], R_[:,p_count].detach(), color=\"tab:green\", linewidth=1, alpha=0.8, label=\"SS-Phys residual\")\n",
    "\n",
    "            p_count += 1\n",
    "            if p_count == n_dof:\n",
    "                break\n",
    "\n",
    "ground_truth = {\n",
    "    \"t_hat\" : t_hat.detach(),\n",
    "    \"x_hat\" : x_hat_gt.detach(),\n",
    "    \"v_hat\" : v_hat_gt.detach(),\n",
    "    \"F_hat\" : F_hat_gt\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"t_hat\" : t_data,\n",
    "    \"x_hat\" : x_data,\n",
    "    \"v_hat\" : v_data,\n",
    "    \"F_hat\" : F_data,\n",
    "    \"sub_ind\" : sub_ind\n",
    "}\n",
    "\n",
    "prediction = {\n",
    "    \"t_hat\" : t_hat,\n",
    "    \"x_hat\" : None,\n",
    "    \"v_hat\" : None,\n",
    "    \"F_hat\" : F_hat_gt\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n",
    "\n",
    "Neural network predicts within a time horizon, given the current state and force:\n",
    "$$\n",
    "\\mathcal{N}_{\\mathbf{z}}(\\mathbf{z}_0,t,\\mathbf{f}_0),~~\\mathcal{N}_{\\mathbf{z}_n}=g_n(\\mathcal{N}_{\\mathbf{z}}) \\qquad \n",
    "\\mathbf{R} = \\partial_t \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A} \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A}_n \\mathcal{N}_{\\mathbf{z}_n} - \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x_0,v_0,t,f_0;\\mathbf{\\theta}) := \\mathcal{L}_{obs} + \\mathcal{L}_{ic} + \\Lambda\\mathcal{L}_{ode}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{obs} = \\langle \\hat{\\mathbf{z}}^* - \\mathcal{N}_{\\hat{\\mathbf{z}}} \\rangle _{\\Omega\\in\\{t=T\\}}\n",
    "% \\mathcal{L}_{cc} = \\sum_{j=1}^{N_{d}} \\left\\langle \\alpha_{\\dot{x}}\\mathcal{N}_{\\hat{\\dot{x}}_j} - \\frac{\\alpha_x}{\\alpha_t}\\partial_{\\hat{t}} \\mathcal{N}_{\\hat{x}_j} \\right\\rangle _{\\Omega_d}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{ic} = \\sum_{j=1}^{N_{d}}\\left[ \n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\frac{\\alpha_x}{\\alpha_t}\\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{x}\\hat{x}_{j,0} - \\alpha_x\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\alpha_{\\dot{x}}\\mathcal{N}_{\\hat{\\dot{x}}_j} \\right\\rangle\n",
    "\\right] _{\\Omega\\in\\{t=0\\}}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{cc} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[j,:] \\right\\rangle _{\\Omega_p}, \\qquad\n",
    "\\mathcal{L}_{ode} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[N_d+j,:] \\right\\rangle _{\\Omega_p}\n",
    "$$\n",
    "where,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{z};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^2_{*}\\bullet = \\frac{\\partial^2\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{t\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{c}\\frac{1}{\\alpha_t}\\partial_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{k} \\hat{x} - \n",
    "\\frac{\\alpha_F}{\\alpha_x} \\hat{F} = 0 \n",
    "\\quad \\rightarrow \\quad \n",
    "\\hat{m} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\hat{c} \\partial_{\\hat{t}}\\hat{x} + \n",
    "\\hat{k}\\hat{x} - \\eta\\hat{F} = 0\n",
    "$$\n",
    "To scale loss function in a physically meaningful way, multiply the loss function by any of the following:\n",
    "$$\n",
    "\\Lambda = 1, \\alpha_t, \\alpha_t^2, \\alpha_x^, \\alpha_F^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_c = 1.0\n",
    "alpha_k = 10.0\n",
    "alpha_kn = 1.0\n",
    "alphas = {\n",
    "    \"c\" : alpha_c,\n",
    "    \"k\" : alpha_k,\n",
    "    \"kn\" : alpha_kn,\n",
    "    \"t\" : alpha_t,\n",
    "    \"x\" : alpha_x,\n",
    "    \"v\" : alpha_v,\n",
    "    \"F\" : alpha_F\n",
    "}\n",
    "\n",
    "nct = 4  # number of collocation points in time window\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "osa_config = {\n",
    "    \"n_input\" : 3*n_dof + 1,\n",
    "    \"n_output\" : 2*n_dof,\n",
    "    \"n_hidden\" : 24,\n",
    "    \"n_layers\" : 6,\n",
    "    \"n_dof\" : n_dof,\n",
    "    \"nct\" : nct,\n",
    "    \"nonlinearity\" : \"cubic\",\n",
    "    \"phys_params\" : {\n",
    "        \"par_type\" : \"constant\",\n",
    "        \"K\": torch.tensor(system.K, dtype=torch.float32),\n",
    "        \"C\" : torch.tensor(system.C, dtype=torch.float32),\n",
    "        \"M\" : torch.tensor(system.M, dtype=torch.float32),\n",
    "        \"Kn\" : torch.tensor(system.Kn, dtype=torch.float32)\n",
    "    },\n",
    "    \"alphas\" : alphas,\n",
    "    \"ode_norm_Lambda\" : alpha_x.item(),\n",
    "    \"forcing\" : F\n",
    "}\n",
    "\n",
    "# configure PINN\n",
    "osa_model = osa_pinn_mdof(osa_config)\n",
    "t_pred = osa_model.set_colls_and_obs(t_data, x_data, v_data, F_data)\n",
    "prediction[\"t_hat\"] = t_pred.detach()\n",
    "\n",
    "# configure optimiser\n",
    "learning_rate = 5e-4\n",
    "betas = (0.99,0.999)\n",
    "optimizer = torch.optim.Adam(osa_model.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    mosaic_key,\n",
    "    figsize=(18,16),\n",
    "    facecolor='w'\n",
    ")\n",
    "\n",
    "print_step = 1000\n",
    "loss_hist=[]\n",
    "lambds = {\n",
    "    'obs' : 10.0,\n",
    "    'ic' : 1.0,\n",
    "    'ode' : 5000.0,\n",
    "    'cc' : 1000.0,\n",
    "}\n",
    "\n",
    "epochs = int(1e6)\n",
    "for i in tqdm(range(epochs)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses = osa_model.loss_func(lambds)\n",
    "    loss_hist.append([loss_it.item() for loss_it in losses] + [loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i+1) % print_step == 0:\n",
    "\n",
    "        z_pred = osa_model.predict()\n",
    "\n",
    "        prediction[\"x_hat\"] = z_pred.detach()[:,:n_dof]\n",
    "        prediction[\"v_hat\"] = z_pred.detach()[:,n_dof:]\n",
    "\n",
    "        plot_result(axs, ground_truth, data, prediction, alphas)\n",
    "\n",
    "        plot_joint_loss_hist(axs[alphabet[2*n_dof]], np.array(loss_hist))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        match osa_model.param_type:\n",
    "            case \"constant\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "            case \"variable\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f\" % (i+1,loss_hist[i][-1],osa_model.phys_params[0]*alphas['c'],osa_model.phys_params[1]*alphas['k']))\n",
    "\n",
    "display.clear_output()\n",
    "match osa_model.param_type:\n",
    "    case \"constant\":\n",
    "        tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "    case \"variable\":\n",
    "        tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f\" % (i+1,loss_hist[i][-1],osa_model.phys_params[0]*alphas['c'],osa_model.phys_params[1]*alphas['k']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = {\n",
    "    'epoch' : i,\n",
    "    # 'gt_system' : system,\n",
    "    'osa_config' : osa_config,\n",
    "    'model' : osa_model.state_dict(),\n",
    "    'loss' : loss_hist,\n",
    "    'ground_truth' : ground_truth,\n",
    "    'data' : data,\n",
    "    'prediction' : prediction,\n",
    "    'alphas' : alphas\n",
    "}\n",
    "\n",
    "torch.save(result_data, 'results/osa_forced_mdof.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
