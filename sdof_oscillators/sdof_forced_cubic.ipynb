{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e966a723",
   "metadata": {},
   "source": [
    "# Cubic stiffness SDOF PINN\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "The example problem we solve here is the 1D damped harmonic oscillator:\n",
    "$$\n",
    "m \\ddot{x} + c \\dot{x} + kx + k_3x^3= F_x~, \\qquad \\ddot{x} + \\tilde{c} \\dot{x} + \\tilde{k}x +\\tilde{k}_3x^3 = \\tilde{F}_x~, \\qquad \\tilde{\\bullet} = \\frac{\\bullet}{m}\n",
    "$$\n",
    "with the initial conditions and forcing,\n",
    "$$\n",
    "x(0) = 0\\mathrm{mm}~~,~~\\dot{x}(0) = 0, \\qquad F_x = F_0\\sin(\\omega_t)\n",
    "$$\n",
    "\n",
    "<!-- Ground truth is simulated using M. Champneys' `mdof-toybox` package -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdof_pinn import bbnn, sdof_pinn, normalise, ParamClipper\n",
    "from sdof_oscillators import sdof_solution, add_noise, generate_excitation\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d3401bd",
   "metadata": {},
   "source": [
    "### Generate Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 1024\n",
    "t = np.linspace(0,60,nt)\n",
    "\n",
    "F0 = 0.01  # N\n",
    "\n",
    "excitation = {\n",
    "    \"type\" : \"sinusoid\",\n",
    "    \"F0\" : F0,\n",
    "    \"w\" : 1.6  # rad/s\n",
    "}\n",
    "\n",
    "# excitation = {\n",
    "#     \"type\" : \"rand_phase_ms\",\n",
    "#     \"F0\" : F0,\n",
    "#     \"freqs\" :   np.array([0.7, 1.8]),\n",
    "#     \"Sx\" :      np.array([1.0, 1.0])\n",
    "# }\n",
    "\n",
    "# excitation = {\n",
    "#     \"type\" : \"sine_sweep\",\n",
    "#     \"F0\" : F0,\n",
    "#     \"w\" :   np.array([0.4, 1.8]),\n",
    "#     \"scale\" :  \"linear\"\n",
    "# }\n",
    "\n",
    "F = generate_excitation(t, **excitation)\n",
    "excitation[\"F\"] = F.reshape(-1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80ee01e6",
   "metadata": {},
   "source": [
    "### Generate training and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10.0\n",
    "c = 1.0\n",
    "k = 20.0\n",
    "k3 = 40.0\n",
    "excitation[\"F_tild\"] = excitation[\"F\"]/m\n",
    "\n",
    "wn = np.sqrt(k/m)\n",
    "zeta = c/(2*m*wn)\n",
    "wd = wn*np.sqrt(1-zeta**2)\n",
    "\n",
    "k_tild = k/m\n",
    "c_tild = c/m\n",
    "k3_tild = k3/m\n",
    "\n",
    "gt_config = {\n",
    "    \"nonlinearity\" : \"cubic\",\n",
    "    # \"m_norm\" : False,\n",
    "    # \"params\" : {\n",
    "    #     \"m\" : m,\n",
    "    #     \"k\" : k,\n",
    "    #     \"c\" : c,\n",
    "    #     \"k3\" : k3\n",
    "    # },\n",
    "    \"m_norm\" : True,\n",
    "    \"params\" : {\n",
    "        \"k_tild\" : k_tild,\n",
    "        \"c_tild\" : c_tild,\n",
    "        \"k3_tild\" : k3_tild\n",
    "    },\n",
    "    \"init_conds\" : {\n",
    "        \"x0\" : 0.0,\n",
    "        \"v0\" : 0.0\n",
    "    },\n",
    "    \"forcing\" : excitation\n",
    "}\n",
    "\n",
    "x, F_tild = sdof_solution(t, **gt_config)\n",
    "x_noisy = add_noise(x.view(-1,1), 0.02)\n",
    "t = torch.tensor(t).view(-1,1).to(torch.float32)\n",
    "\n",
    "# slice out a number of points from the start of the signal\n",
    "sub_ind = np.arange(0, int(nt/3), 1)\n",
    "\n",
    "t_data = t[sub_ind]\n",
    "x_data = x_noisy[sub_ind]\n",
    "\n",
    "fig, axs= plt.subplots(2,1)\n",
    "axs[0].plot(t, x_noisy, color=\"gray\", label=\"Noisy data\")\n",
    "axs[0].plot(t, x, label=\"Exact solution\")\n",
    "axs[0].scatter(t_data, x_data, color=\"tab:orange\", label=\"Training data\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(t, F_tild, color=\"red\", label=\"Forcing\")\n",
    "axs[1].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c273a51f",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47897ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hat, alpha_t = normalise(t, \"range\")\n",
    "x_hat, alpha_x = normalise(x_noisy, \"range\")\n",
    "F_hat, alpha_F = normalise(F_tild, \"range\")\n",
    "x_hat_gt = x/alpha_x\n",
    "t_data = t_hat[sub_ind]\n",
    "x_data = x_hat[sub_ind]\n",
    "alphas = {\n",
    "    \"t\" : alpha_t,\n",
    "    \"x\" : alpha_x\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_hat, x_hat, color=\"gray\", label=\"Noisy data\")\n",
    "ax.plot(t_hat, x_hat_gt, label=\"Exact solution\")\n",
    "ax.scatter(t_data, x_data, color=\"tab:orange\", label=\"Training data\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d8cd505",
   "metadata": {},
   "source": [
    "## 'Black-box' network\n",
    "\n",
    "$$\n",
    "\\textrm{argmin}\\mathcal{L}(t;\\mathbf{\\theta}) := \\langle \\hat{x}^* - \\mathcal{N}_x \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847195e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_result(ax, t_hat, x_hat, t_data, x_data, tpred, xpred, alphas):\n",
    "    ax.cla()\n",
    "    ax.plot(t_hat*alphas[\"t\"], x_hat*alphas[\"x\"], color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n",
    "    ax.plot(tpred*alphas[\"t\"], xpred*alphas[\"x\"], color=\"tab:blue\", linewidth=4, alpha=0.8, label=\"Neural network prediction\")\n",
    "    ax.scatter(t_data*alphas[\"t\"], x_data*alphas[\"x\"], s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "    l = ax.legend(frameon=False, fontsize=\"large\")\n",
    "    xL = torch.amax(t_hat)*alphas[\"t\"]\n",
    "    yL = torch.amax(torch.abs(x_hat))*alphas[\"x\"]\n",
    "    ax.set_xlim(-0.05*xL, 1.05*xL)\n",
    "    ax.set_ylim(-1.1*yL, 1.1*yL)\n",
    "\n",
    "def plot_loss_hist(ax,loss_hist):\n",
    "    ax.cla()\n",
    "    n_epoch = len(loss_hist)\n",
    "    ax.plot(np.arange(1,n_epoch+1),loss_hist,'b')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "# train standard neural network to fit training data\n",
    "torch.manual_seed(123)\n",
    "epochs = int(50e3)\n",
    "bb_model = bbnn(1,1,32,2)\n",
    "optimizer = torch.optim.Adam(bb_model.parameters(),lr=1e-3)\n",
    "loss_hist = []\n",
    "fig, axs = plt.subplots(1,2,figsize=(18,6),facecolor='w')\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = bb_model.loss_func(t_data, x_data)\n",
    "    loss_hist.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % 500 == 0:\n",
    "        \n",
    "        xpred = bb_model(t_hat).detach()\n",
    "        \n",
    "        plot_result(axs[0], t_hat, x_hat_gt, t_data, x_data, t_hat, xpred, alphas)\n",
    "\n",
    "        plot_loss_hist(axs[1],loss_hist)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285d57ca",
   "metadata": {},
   "source": [
    "## PINN\n",
    "\n",
    "$$\n",
    "\\textrm{argmin}\\mathcal{L}(t;\\mathbf{\\theta}) = \\mathcal{L}_a + \\Lambda\\mathcal{L}_{ode}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_a = \\langle \\hat{x}^* - \\mathcal{N}_{\\hat{x}} \\rangle _{\\Omega_a}, \\qquad\n",
    "\\mathcal{L}_{ode} = \\langle \\hat{m} \\partial^2_{\\hat{t}}\\mathcal{N}_{\\hat{x}} + \\hat{c} \\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}} + \\hat{k}\\mathcal{N}_{\\hat{x}} + \\hat{k}_3\\mathcal{N}_{\\hat{x}}^3 - \\eta \\hat{F}\\rangle _{\\Omega_d}\n",
    "$$\n",
    "where,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{x};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^2_{*}\\bullet = \\frac{\\partial^2\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{x\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{c}\\frac{1}{\\alpha_t}\\partial_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{k} \\hat{x} +\n",
    "\\tilde{k}_3 \\alpha_x^2 \\hat{x}^3 - \n",
    "\\frac{\\alpha_F}{\\alpha_x} \\hat{F} = 0 \n",
    "\\quad \\rightarrow \\quad \n",
    "\\hat{m} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\hat{c} \\partial_{\\hat{t}}\\hat{x} + \n",
    "\\hat{k}\\hat{x} + \\hat{k}_3\\hat{x}^3\n",
    "- \\eta\\hat{F} = 0\n",
    "$$\n",
    "To scale loss function in a physically meaningful way, multiply the loss function by any of the following:\n",
    "$$\n",
    "\\Lambda = 1, \\alpha_t, \\alpha_t^2, \\alpha_x^, \\alpha_F^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc42d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_c = c_tild*10\n",
    "alpha_k = k_tild*10\n",
    "alpha_k3 = k3_tild*10\n",
    "alphas = {\n",
    "    \"c\" : alpha_c,\n",
    "    \"k\" : alpha_k,\n",
    "    \"k3\" : alpha_k3,\n",
    "    \"t\" : alpha_t,\n",
    "    \"x\" : alpha_x,\n",
    "    \"F\" : alpha_F\n",
    "}\n",
    "\n",
    "def plot_joint_loss_hist(ax,loss_hist):\n",
    "    n_epoch = len(loss_hist)\n",
    "    ax.cla()\n",
    "    ax.plot(np.arange(1,n_epoch+1),loss_hist[:,0],'b',label=\"Observation loss\")\n",
    "    ax.plot(np.arange(1,n_epoch+1),loss_hist[:,1],'r',label=\"Physics loss\")\n",
    "    ax.plot(np.arange(1,n_epoch+1),loss_hist[:,2],'k--',label=\"Total loss\")\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "torch.manual_seed(123)\n",
    "pi_model = sdof_pinn(\n",
    "    N_INPUT = 1,\n",
    "    N_OUTPUT = 1,\n",
    "    N_HIDDEN = 32,\n",
    "    N_LAYERS = 2\n",
    "    )\n",
    "\n",
    "pinn_config = {\n",
    "    \"nonlinearity\" : \"cubic\",\n",
    "    \"phys_params\" : {\n",
    "        \"par_type\" : \"constant\",\n",
    "        \"k\": k_tild,\n",
    "        \"c\" : c_tild,\n",
    "        \"k3\" : k3_tild\n",
    "    },\n",
    "    \"alphas\" : alphas,\n",
    "    \"ode_norm_Lambda\" : 1.0,\n",
    "    \"forcing\" : excitation\n",
    "}\n",
    "\n",
    "# configure pinn\n",
    "pi_model.configure(**pinn_config)\n",
    "\n",
    "ntp = nt  # number of collocation points\n",
    "t_physics = torch.linspace(0, torch.max(t_hat), ntp).view(-1,1).requires_grad_()\n",
    "\n",
    "epochs = int(20e3)\n",
    "betas = (0.99,0.999)\n",
    "optimizer = torch.optim.Adam(pi_model.parameters(),lr=1e-2,betas=betas)\n",
    "\n",
    "clipper = ParamClipper()\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(18,6),facecolor='w')\n",
    "loss_hist=[]\n",
    "lambds = {\n",
    "    'obs' : torch.tensor(1.0),\n",
    "    'ode' : torch.tensor(1e-1)\n",
    "}\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss, losses = pi_model.loss_func(t_physics, t_data, x_data, lambds)\n",
    "\n",
    "    loss_hist.append([losses[0].item(), losses[1].item(), loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step(loss)\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % 500 == 0: \n",
    "        \n",
    "        xpred = pi_model(t_physics).detach()\n",
    "        \n",
    "        plot_result(axs[0], t_hat, x_hat_gt, t_data, x_data, t_physics.detach(), xpred, alphas)\n",
    "\n",
    "        plot_joint_loss_hist(axs[1],np.array(loss_hist))\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        match pi_model.param_type:\n",
    "            case \"constant\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss))\n",
    "            case \"variable\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f ---- k3_tild: %.4f\" % (i+1,loss,pi_model.phys_params[0]*alphas['c'],pi_model.phys_params[1]*alphas['k'],pi_model.phys_params[2]*alphas['k3']))\n",
    "\n",
    "display.clear_output()\n",
    "match pi_model.param_type:\n",
    "            case \"constant\":\n",
    "                tqdma.write(\"Epochs : %d ---- Loss: %.2e\" % (i+1,loss))\n",
    "            case \"variable\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f ---- k3_tild: %.4f\" % (i+1,loss,pi_model.phys_params[0]*alphas['c'],pi_model.phys_params[1]*alphas['k'],pi_model.phys_params[2]*alphas['k3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch' : i,\n",
    "    'model' : pi_model.state_dict(),\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "    'loss' : loss_hist,\n",
    "    # 'scheduler' : scheduler\n",
    "}\n",
    "torch.save(checkpoint,'checkpoints/sdof_forced_cubic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86984682",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/sdof_forced_cubic.pth')\n",
    "loss_hist = checkpoint[\"loss\"]\n",
    "pi_model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 1e-2\n",
    "    # g['betas'] = (0.98,0.999)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(18,6),facecolor='w')\n",
    "epochs = int(2000e3)\n",
    "\n",
    "lambds = {\n",
    "    'obs' : torch.tensor(1.0),\n",
    "    'ode' : torch.tensor(1e1)\n",
    "}\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss, losses = pi_model.loss_func(t_physics, t_data, x_data, lambds)\n",
    "\n",
    "    loss_hist.append([losses[0].item(), losses[1].item(), loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # pi_model.apply(clipper)\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % 500 == 0: \n",
    "        \n",
    "        xpred = pi_model(t_physics).detach()\n",
    "        \n",
    "        plot_result(axs[0], t_hat, x_hat_gt, t_data, x_data, t_physics.detach(), xpred, alphas)\n",
    "\n",
    "        plot_joint_loss_hist(axs[1],np.array(loss_hist))\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        match pi_model.param_type:\n",
    "            case \"constant\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss))\n",
    "            case \"variable\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f ---- k3_tild: %.4f\" % (i+1,loss,pi_model.phys_params[0]*alphas['c'],pi_model.phys_params[1]*alphas['k'],pi_model.phys_params[2]*alphas['k3']))\n",
    "\n",
    "display.clear_output()\n",
    "match pi_model.param_type:\n",
    "            case \"constant\":\n",
    "                tqdma.write(\"Epochs : %d ---- Loss: %.2e\" % (i+1,loss))\n",
    "            case \"variable\":\n",
    "                tqdma.write(\"Epoch : %d ---- Loss: %.2e \\nc_tild: %.4f ---- k_tild: %.4f ---- k3_tild: %.4f\" % (i+1,loss,pi_model.phys_params[0]*alphas['c'],pi_model.phys_params[1]*alphas['k'],pi_model.phys_params[2]*alphas['k3']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygplates310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae34e4d16e0e7f63267c344521c8d66f375234259a32056c49f7d19b526eb780"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
