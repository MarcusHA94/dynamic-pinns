{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDOF PINN - instance prediction\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "The example problem we solve here is the 3DOF nonlinear-stiffness oscillator defined in state space:\n",
    "$$\n",
    "\\dot{\\mathbf{z}} = \\mathbf{A}\\mathbf{z} + \\mathbf{A}_n\\mathbf{z}_n + \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "\\mathbf{z} = \\left\\{ x_1, x_2, ... , x_n, \\dot{x}_1, \\dot{x}_2, ... , \\dot{x}_n \\right\\}^T, \\quad\n",
    "\\mathbf{f} = \\left\\{ f_1, f_2, ... , f_n \\right\\}^T\n",
    "$$\n",
    "and $\\mathbf{z}_n$ is the nonlinear state vector.\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 0 & \\mathbf{I} \\\\ -\\mathbf{M}^{-1}\\mathbf{K} & -\\mathbf{M}^{-1}\\mathbf{C} \\end{bmatrix}, \\quad\n",
    "\\mathbf{A}_n = \\begin{bmatrix} 0 \\\\ -\\mathbf{M}^{-1} \\mathbf{K}_n \\end{bmatrix}, \\quad\n",
    "\\mathbf{H} = \\begin{bmatrix} 0 \\\\ \\mathbf{M}^{-1} \\end{bmatrix}\n",
    "$$\n",
    "with the initial conditions\n",
    "$$\n",
    "\\mathbf{x}(0) = \\mathbf{x}_0~~,~~\\dot{\\mathbf{x}}(0) = \\dot{\\mathbf{x}}_0\n",
    "$$\n",
    "\n",
    "As an example, for a 3DOF system with cubic nonlinearities, fixed at the first degree of freedom:\n",
    "$$\n",
    "\\mathbf{z}_n = g_n(\\mathbf{z}) = \\left\\{ x_1^3, (x_2-x_1)^3, (x_3-x_2)^3 \\right\\}^T, \\quad\n",
    "\\mathbf{K}_n = \\begin{bmatrix} k_{n,1} & -k_{n,2} & 0 \\\\ 0 & k_{n,2} & -k_{n,3} \\\\ 0 & 0 & k_{n,3} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mdof_pinn_batch import mdof_pinn_stoch, mdof_dataset, ParamClipper\n",
    "import dynasim\n",
    "from mdof_solutions import gen_ndof_cantilever, add_noise\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.stats import qmc\n",
    "\n",
    "import string\n",
    "import findiff\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 512\n",
    "time = np.linspace(0,30,nt)\n",
    "\n",
    "F0 = 1.0  # N\n",
    "n_dof = 2\n",
    "\n",
    "# set physical parameters\n",
    "k1 = 10.0\n",
    "c1 = 0.25\n",
    "m1 = 1.0\n",
    "kn_ = np.zeros((n_dof))\n",
    "kn_[0] = 100.0\n",
    "\n",
    "# create nonlinearity\n",
    "cubic_nonlin = dynasim.nonlinearities.exponent_stiffness(kn_, exponent=3, dofs=n_dof)\n",
    "\n",
    "# instantiate system\n",
    "system = dynasim.systems.cantilever(m1, c1, k1, dofs=n_dof, nonlinearity=cubic_nonlin)\n",
    "\n",
    "true_params = {\n",
    "    'm_' : system.m_,\n",
    "    'c_' : system.c_,\n",
    "    'k_' : system.k_,\n",
    "    'kn_' : kn_\n",
    "}\n",
    "\n",
    "# generate excitations\n",
    "# system.excitations = [\n",
    "#     dynasim.actuators.rand_phase_ms(\n",
    "#         freqs = np.array([0.7, 0.85, 1.6, 1.8]),\n",
    "#         Sx = np.ones(4)\n",
    "#     ), None]\n",
    "system.excitations = [\n",
    "    dynasim.actuators.sine_sweep(\n",
    "        w_l = 0.7,\n",
    "        w_u = 4.0,\n",
    "        F0 = 1.0\n",
    "    ), None]\n",
    "\n",
    "x0 = np.array([-2.0, 0.0, 3.0])\n",
    "v0 = np.array([-2.0, 0.0, 0.0])\n",
    "z0 = np.concatenate((x0, v0), axis=0)\n",
    "\n",
    "data = system.simulate(time, z0=None)\n",
    "\n",
    "t_span = time.reshape(-1,1)\n",
    "xx, vv = data['x'].T, data['xdot'].T\n",
    "f = system.f.T\n",
    "xx_noisy, vv_noisy = add_noise(xx, db=-30, seed=43810), add_noise(vv, db=-30, seed=13927)\n",
    "f_noisy = add_noise(f, db=-30, seed=1234)\n",
    "\n",
    "ground_truth = {\n",
    "    \"t\" : t_span,\n",
    "    \"x_hat\" : xx,\n",
    "    \"v_hat\" : vv,\n",
    "    \"f_hat\" : f\n",
    "}\n",
    "\n",
    "if n_dof > 4:\n",
    "    sub_rows = n_dof // 4 + int((n_dof%4)!=0)\n",
    "    sub_cols = 4\n",
    "else:\n",
    "    sub_rows = 1\n",
    "    sub_cols = n_dof\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(8*sub_cols, 8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(time, xx[:,p_count], color=\"tab:blue\", label=\"Displacement\", linewidth=1.0, linestyle='--')\n",
    "        # axs[j*3,i].grid()\n",
    "        axs[j*3,i].legend()\n",
    "\n",
    "        axs[j*3+1,i].plot(time, vv[:,p_count], color=\"tab:red\", label=\"Velocity\", linewidth=1.0, linestyle='--')\n",
    "        # axs[j*3+1,i].grid()\n",
    "        axs[j*3+1,i].legend()\n",
    "\n",
    "        axs[j*3+2,i].plot(time, f[:,p_count], color=\"tab:gray\", label=\"Forcing\", linewidth=1.0, linestyle='--')\n",
    "        axs[j*3+2,i].legend()\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise and create some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "subsample = 1\n",
    "dataset = mdof_dataset(xx_noisy, vv_noisy, f_noisy, t_span, subsample)\n",
    "phases = ['full', 'train', 'val']\n",
    "full_dataset = torch.utils.data.random_split(dataset, [1.0])\n",
    "train_size = 1.0; val_size = 0.0\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)#, multiprocessing_context='fork')\n",
    "# datasets = {\n",
    "#     'full' : full_dataset,\n",
    "#     'train' : train_dataset,\n",
    "#     'val' : val_dataset\n",
    "# }\n",
    "# dataloaders = {\n",
    "#     phase: DataLoader(dataset=datasets[phase], batch_size=batch_size, shuffle=True if phase=='train' else False, num_workers=8) for phase in phases\n",
    "# }\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(8*sub_cols,8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['x_hat'][:,i]/dataset.alphas['x'], color=\"tab:blue\", linewidth=1.0, linestyle='-')\n",
    "        # axs[j*3,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['x'][:,i]/dataset.alphas['x']-dataset.data[:, i], color=\"tab:blue\", linewidth=1.0, linestyle='-')\n",
    "        axs[j*3,i].scatter(dataset.data[:, -1], dataset.data[:, i], color=\"tab:blue\", s=6)\n",
    "        axs[j*3,i].grid()\n",
    "\n",
    "        axs[j*3+1,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['v_hat'][:,i]/dataset.alphas['v'], color=\"tab:red\", linewidth=1.0, linestyle='-')\n",
    "        # axs[j*3+1,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['v'][:,i]/dataset.alphas['v']-dataset.data[:, n_dof + i], color=\"tab:red\", linewidth=1.0, linestyle='-')\n",
    "        axs[j*3+1,i].scatter(dataset.data[:, -1], dataset.data[:, n_dof + i], color=\"tab:red\", s=6)\n",
    "        axs[j*3+1,i].grid()\n",
    "\n",
    "        axs[j*3+2,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['f_hat'][:,i]/dataset.alphas['f'], color=\"tab:gray\", linewidth=1.0, linestyle='-')\n",
    "        # axs[j*3+2,i].plot(ground_truth['t'][:,0]/dataset.alphas['t'], ground_truth['f'][:,i]/dataset.alphas['f']-dataset.data[:, 2 * n_dof + i], color=\"tab:gray\", linewidth=1.0, linestyle='-')\n",
    "        axs[j*3+2,i].scatter(dataset.data[:, -1], dataset.data[:, 2 * n_dof + i], color=\"tab:gray\", s=6)\n",
    "        axs[j*3+2,i].grid()\n",
    "\n",
    "        p_count += 1\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_uppercase)\n",
    "mosaic_key = ''\n",
    "alph_count = 0\n",
    "for j in range(sub_rows):\n",
    "    mosaic_key += ''.join(alphabet[alph_count:alph_count+sub_cols]) + ';' + ''.join(alphabet[alph_count+sub_cols:alph_count+2*sub_cols]) + ';' + ''.join(alphabet[alph_count+2*sub_cols:alph_count+3*sub_cols]) + ';'\n",
    "    alph_count += 3*sub_cols\n",
    "mosaic_key += ''.join([alphabet[alph_count]]*sub_cols)\n",
    "# print(mosaic_key)\n",
    "\n",
    "def plot_joint_loss_hist(ax,loss_hist):\n",
    "    n_epoch = len(loss_hist)\n",
    "    indices = np.arange(1,n_epoch+1)\n",
    "    if n_epoch > 20000:\n",
    "        step = int(np.floor(n_epoch/10000))\n",
    "        loss_hist = loss_hist[::step,:]\n",
    "        indices = indices[::step]\n",
    "    labels = [\"L_obs\", \"L_cc\", \"L_ode\", \"L\"]\n",
    "    colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"black\"]\n",
    "    ax.cla()\n",
    "    for i in range(len(labels)):\n",
    "        ax.plot(indices, loss_hist[:,i], color=colors[i], label=labels[i])\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_result(axs_m, ground_truth, data, prediction, alphas):\n",
    "    for ax in axs_m:\n",
    "        axs_m[ax].cla()\n",
    "    axs_top_list = []\n",
    "    for j in range(sub_rows):\n",
    "        axs_top_list.append([axs_m[alphabet[3*sub_cols*j+i]] for i in range(sub_cols)])\n",
    "        axs_top_list.append([axs_m[alphabet[3*sub_cols*j+sub_cols+i]] for i in range(sub_cols)])\n",
    "        axs_top_list.append([axs_m[alphabet[3*sub_cols*j+2*sub_cols+i]] for i in range(sub_cols)])\n",
    "    axs_top = np.array(axs_top_list)\n",
    "\n",
    "    plot_keys = [\"x_hat\", \"v_hat\", \"f_hat\"]\n",
    "    plot_cols = [\"tab:blue\", \"tab:red\", \"tab:gray\"]\n",
    "    alpha_keys = [\"x\", \"v\", \"f\"]\n",
    "    p_count = 0\n",
    "    for j in range(sub_rows):\n",
    "        for i in range(sub_cols):\n",
    "            for n in range(3):\n",
    "                # axs_top[j*3+n,i].plot(data[\"t_hat\"].detach()*alphas[\"t\"], data[plot_keys[n]][:,p_count].detach()*alphas[alpha_keys[n]], color=\"tab:olive\", linewidth=1, alpha=0.8, label='Training data')\n",
    "                axs_top[j*3+n,i].plot(ground_truth[\"t\"], ground_truth[plot_keys[n]][:,p_count], color=\"grey\", linewidth=2, alpha=0.5, label=\"Exact solution\")\n",
    "                axs_top[j*3+n,i].plot(prediction[\"t_hat\"]*alphas[\"t\"], prediction[plot_keys[n]][:,p_count]*alphas[alpha_keys[n]], color=plot_cols[n], linewidth=2, alpha=0.8, linestyle='--', label=\"Neural network prediction\")\n",
    "                if n < 2:\n",
    "                    axs_top[j*3+n,i].fill_between((prediction[\"t_hat\"]*alphas[\"t\"]).squeeze(), (prediction[plot_keys[n]][:,p_count]-2*prediction['sigma'])*alphas[alpha_keys[n]], (prediction[plot_keys[n]][:,p_count]+2*prediction['sigma'])*alphas[alpha_keys[n]], alpha=0.25, color=\"tab:blue\", label=r\"$2\\sigma$ Range\")\n",
    "                else:\n",
    "                    axs_top[j*3+n,i].fill_between((prediction[\"t_hat\"]*alphas[\"t\"]).squeeze(), (prediction[plot_keys[n]][:,p_count]-2*prediction['sigma_s'])*alphas[alpha_keys[n]], (prediction[plot_keys[n]][:,p_count]+2*prediction['sigma_s']), alpha=0.25, color=\"tab:blue\", label=r\"$2\\sigma$ Range\")\n",
    "                xL = np.amax(ground_truth[\"t\"])\n",
    "                yL = np.amax(np.abs(ground_truth[plot_keys[n]][:,p_count]))\n",
    "                axs_top[j*2+n,i].set_xlim(-0.05*xL, 1.05*xL)\n",
    "                axs_top[j*2+n,i].set_ylim(-1.1*yL, 1.1*yL)\n",
    "\n",
    "            p_count += 1\n",
    "            if p_count == n_dof:\n",
    "                break\n",
    "\n",
    "prediction = {\n",
    "    \"t_hat\" : time,\n",
    "    \"x_hat\" : None,\n",
    "    \"v_hat\" : None,\n",
    "    \"F_hat\" : None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(vec2sort: np.ndarray, *data_: tuple[np.ndarray,...]):\n",
    "    sort_ids = np.argsort(vec2sort)\n",
    "    sorted_data_ = [None] * len(data_)\n",
    "    for i, data in enumerate(data_):\n",
    "        sorted_data_[i] = np.zeros_like(data)\n",
    "        if len(data.shape) > 1:\n",
    "            for j in range(data.shape[1]):\n",
    "                sorted_data_[i][:,j] = data[sort_ids,j].squeeze()\n",
    "        else:\n",
    "            sorted_data_[i] = data[sort_ids]\n",
    "    if len(data_) > 1:\n",
    "        return tuple(sorted_data_), sort_ids\n",
    "    else:\n",
    "        return sorted_data_[0], sort_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n",
    "\n",
    "Neural network predicts over the full time domain:\n",
    "$$\n",
    "\\mathcal{N}_{\\mathbf{z}}(t), \\qquad \n",
    "\\mathbf{R} = \\partial_t \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A} \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A}_n \\mathcal{N}_{\\mathbf{z}_n} - \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(t;\\mathbf{\\theta}) := \\mathcal{L}_{obs} + \\mathcal{L}_{ic} + \\Lambda\\mathcal{L}_{ode}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{obs} = \\langle \\hat{\\mathbf{z}}^* - \\mathcal{N}_{\\hat{\\mathbf{z}}} \\rangle _{\\Omega_o}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{obs} = \\prod_{i=1}^{N} -\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{||\\hat{\\mathbf{z}}^* - \\mathcal{N}_{\\hat{\\mathbf{z}}}||^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "in the log space\n",
    "$$\n",
    "\\mathcal{L}_{obs} = -N\\log(\\sigma) - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^{N} \\frac{||\\hat{\\mathbf{z}}^* - \\mathcal{N}_{\\hat{\\mathbf{z}}}||^2}{\\sigma^2}\n",
    "$$\n",
    "<!-- $$\n",
    "\\mathcal{L}_{ic} = \\sum_{j=1}^{N_{d}}\\left[ \n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\frac{\\alpha_x}{\\alpha_t}\\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{x}\\hat{x}_{j,0} - \\alpha_x\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\alpha_{\\dot{x}}\\mathcal{N}_{\\hat{\\dot{x}}_j} \\right\\rangle\n",
    "\\right] _{\\Omega\\in\\{t=0\\}}\n",
    "$$ -->\n",
    "$$\n",
    "\\mathcal{L}_{cc} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[j,:] \\right\\rangle _{\\Omega_p}, \\qquad\n",
    "\\mathcal{L}_{ode} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[N_d+j,:] \\right\\rangle _{\\Omega_p}\n",
    "$$\n",
    "where,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{z};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^2_{*}\\bullet = \\frac{\\partial^2\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{t\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{c}\\frac{1}{\\alpha_t}\\partial_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{k} \\hat{x} - \n",
    "\\frac{\\alpha_F}{\\alpha_x} \\hat{F} = 0 \n",
    "\\quad \\rightarrow \\quad \n",
    "\\hat{m} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\hat{c} \\partial_{\\hat{t}}\\hat{x} + \n",
    "\\hat{k}\\hat{x} - \\eta\\hat{F} = 0\n",
    "$$\n",
    "To scale loss function in a physically meaningful way, multiply the loss function by any of the following:\n",
    "$$\n",
    "\\Lambda = 1, \\alpha_t, \\alpha_t^2, \\alpha_x^, \\alpha_F^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {\n",
    "    \"c\" : 1.0,\n",
    "    \"k\" : 1.0,\n",
    "    \"kn\" : 1.0,\n",
    "    \"sigma\" : 1.0,\n",
    "    \"sigma_s\" : 1.0\n",
    "}\n",
    "alphas.update(dataset.alphas)\n",
    "\n",
    "nct = nt  # number of collocation points\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "pinn_config = {\n",
    "    \"n_input\" : 1,\n",
    "    \"n_output\" : 2*n_dof,\n",
    "    \"n_hidden\" : 16,\n",
    "    \"n_layers\" : 4,\n",
    "    \"n_dof\" : n_dof,\n",
    "    \"nct\" : nct,\n",
    "    \"nonlinearity\" : \"cubic\",\n",
    "    \"phys_params\" : {\n",
    "        \"m_\" : {\n",
    "            \"type\" : \"constant\",\n",
    "            \"value\" : torch.tensor(system.m_, dtype=torch.float32)\n",
    "        },\n",
    "        \"c_\" : {\n",
    "            \"type\" : \"constant\",\n",
    "            \"value\" : torch.tensor(system.c_, dtype=torch.float32)\n",
    "        },\n",
    "        \"k_\" : {\n",
    "            \"type\" : \"constant\",\n",
    "            \"value\" : torch.tensor(system.k_, dtype=torch.float32)\n",
    "        },\n",
    "        \"kn_\" : {\n",
    "            \"type\" : \"constant\",\n",
    "            \"value\" : torch.tensor(kn_, dtype=torch.float32)\n",
    "        },\n",
    "        \"sigma_\" : {\n",
    "            \"type\" : \"variable\",\n",
    "            \"value\" : torch.tensor(1.0, dtype=torch.float32)\n",
    "        },\n",
    "        \"sigma_s_\" : {\n",
    "            \"type\" : \"variable\",\n",
    "            \"value\" : torch.tensor(1.0, dtype=torch.float32)\n",
    "        },\n",
    "    },\n",
    "    \"param_func\" : gen_ndof_cantilever,\n",
    "    \"alphas\" : alphas,\n",
    "    \"forcing\" : f\n",
    "}\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# configure PINN\n",
    "mdof_model = mdof_pinn_stoch(pinn_config, device)\n",
    "mdof_model = mdof_model.to(device)\n",
    "\n",
    "# configure optimiser\n",
    "learning_rate = 2.5e-3\n",
    "betas = (0.99,0.999)\n",
    "optimizer = torch.optim.Adam(mdof_model.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "clipper = ParamClipper()\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    mosaic_key,\n",
    "    figsize=(18,16),\n",
    "    facecolor='w'\n",
    ")\n",
    "axs2 = axs.copy()\n",
    "for key, ax in axs.items():\n",
    "    axs2[key] = ax.twinx()\n",
    "\n",
    "print_step = 200\n",
    "loss_hist=[]\n",
    "lambds = {\n",
    "    'obs' : 1.0e-3,\n",
    "    'ic' : 0.0,\n",
    "    'ode' : 0.0e-4,\n",
    "    'cc' : 0.0\n",
    "}\n",
    "\n",
    "mdof_model.set_switches(lambds)\n",
    "# compiled_model = mdof_model.to(device)\n",
    "# compiled_model = torch.compile(mdof_model, mode=\"reduce-overhead\")\n",
    "# compiled_model = torch.compile(mdof_model, mode=\"max-autotune\").to(device)\n",
    "\n",
    "num_obs_samps = len(train_dataset)\n",
    "num_col_samps = len(train_dataset) * subsample\n",
    "z_pred = np.zeros((num_col_samps, 2*n_dof))\n",
    "f_pred = np.zeros((num_col_samps, n_dof))\n",
    "t_pred = np.zeros((num_col_samps, 1))\n",
    "\n",
    "epochs = int(2e6)\n",
    "epoch = 0\n",
    "progress_bar = tqdm(total=epochs)\n",
    "phases = ['train', 'val']\n",
    "while epoch < epochs:\n",
    "    write_string = ''\n",
    "    write_string += 'Epoch {}\\n'.format(epoch)\n",
    "    phase_loss = 0.\n",
    "    losses = [0.0] * 3\n",
    "    for i, (obs_data, col_data) in enumerate(train_loader):\n",
    "        # parse data sample\n",
    "        state_obs = obs_data[..., :2*n_dof].float().to(device).requires_grad_()\n",
    "        time_obs = obs_data[..., -1].reshape(-1,1).float().to(device).requires_grad_()\n",
    "\n",
    "        force_col = col_data[..., 2*n_dof:3*n_dof].float().to(device).requires_grad_()\n",
    "        force_col = force_col.reshape(-1, n_dof)  # unroll collocation data\n",
    "        time_col = col_data[..., -1].float().to(device).requires_grad_()\n",
    "        time_col = time_col.reshape(-1, 1)  # unroll collocation data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, losses_i, _ = mdof_model.loss_func(lambds, time_obs, state_obs, time_col, force_col)\n",
    "        phase_loss += loss.item()\n",
    "        losses = [losses[j] + loss_i for j, loss_i in enumerate(losses_i)]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_hist.append([loss_it.item() for loss_it in losses] + [phase_loss])\n",
    "    write_string += '\\tLoss {:.4e}\\n'.format(phase_loss)\n",
    "\n",
    "    for phase in phases:\n",
    "        a = 0\n",
    "        # phase_loss = 0.\n",
    "        # losses = [0.0] * 3\n",
    "        # write_string += '\\tPhase {}\\n'.format(phase)\n",
    "        # if phase == 'train':\n",
    "        #     compiled_model.train()\n",
    "        # else:\n",
    "        #     compiled_model.eval()\n",
    "        # for i, (obs_data, col_data) in enumerate(dataloaders[phase]):\n",
    "        #     # parse data sample\n",
    "        #     state_obs = obs_data[..., :2*n_dof].to(device).float().requires_grad_()\n",
    "        #     time_obs = obs_data[..., -1].reshape(-1,1).to(device).float().requires_grad_()\n",
    "\n",
    "        #     force_col = col_data[..., 2*n_dof:3*n_dof].to(device).float().requires_grad_()\n",
    "        #     force_col = force_col.reshape(-1, n_dof)  # unroll collocation data\n",
    "        #     time_col = col_data[..., -1].to(device).float().requires_grad_()\n",
    "        #     time_col = time_col.reshape(-1, 1)  # unroll collocation data\n",
    "\n",
    "        #     if phase == 'train':\n",
    "        #         optimizer.zero_grad()\n",
    "        #     loss, losses_i, _ = compiled_model.loss_func(lambds, time_obs, state_obs, time_col, force_col)\n",
    "        #     phase_loss += loss.item()\n",
    "        #     losses = [losses[j] + loss_i for j, loss_i in enumerate(losses_i)]\n",
    "        #     if phase == 'train':\n",
    "        #         loss.backward()\n",
    "        #         optimizer.step()\n",
    "        # if phase == 'train':\n",
    "        #     loss_hist.append([loss_it.item() for loss_it in losses] + [phase_loss])\n",
    "        # write_string += '\\tLoss {:.4e}\\n'.format(phase_loss)\n",
    "    \n",
    "    if (epoch+1) % print_step == 0:\n",
    "\n",
    "        for i, (obs_data, col_data) in enumerate(train_loader):\n",
    "\n",
    "            inpoint_ = i * batch_size * subsample\n",
    "            outpoint_ = (i + 1) * batch_size * subsample\n",
    "\n",
    "            t_col = col_data[..., -1].to(device).float().requires_grad_()\n",
    "            pred_inputs = t_col.reshape(-1, 1)\n",
    "            z_pred_, f_pred_ = mdof_model.predict(pred_inputs)\n",
    "            t_pred[inpoint_:outpoint_] = pred_inputs.detach().cpu().numpy()\n",
    "            z_pred[inpoint_:outpoint_, :], f_pred[inpoint_:outpoint_, :] = z_pred_.detach().cpu().reshape(-1, 2*n_dof).numpy(), f_pred_.detach().cpu().reshape(-1, n_dof).numpy()\n",
    "        \n",
    "        (z_pred, f_pred, t_pred), _ = sort_data(t_pred[:,0], z_pred, f_pred, t_pred)\n",
    "\n",
    "        prediction['t_hat'] = t_pred\n",
    "        prediction[\"x_hat\"] = z_pred[:,:n_dof]\n",
    "        prediction[\"v_hat\"] = z_pred[:,n_dof:]\n",
    "        prediction[\"f_hat\"] = f_pred\n",
    "        prediction['sigma'] = mdof_model.sigma_.detach().item()\n",
    "        prediction['sigma_s'] = mdof_model.sigma_s_.detach().item()\n",
    "\n",
    "        plot_result(axs, ground_truth, data, prediction, alphas)#, residuals['R_ode'], axs2)\n",
    "\n",
    "        plot_joint_loss_hist(axs[alphabet[3*n_dof]], np.array(loss_hist))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        write_string += 'sigma_o: {:.4e} ---- sigma_p: {:.4e}\\n'.format(mdof_model.sigma_.detach(), mdof_model.sigma_s_.detach())\n",
    "        write_string += 'c :                       k :                     kn : \\n'\n",
    "        for j in range(n_dof):\n",
    "            wri_str = '{} : '.format(j+1)\n",
    "            for param in ['c_','k_','kn_']:\n",
    "                if pinn_config['phys_params'][param]['type']=='constant':\n",
    "                    wri_str += '{:.4f} '.format(getattr(mdof_model,param)[j])\n",
    "                elif pinn_config['phys_params'][param]['type']=='variable':\n",
    "                    wri_str += '{:.4f} '.format(getattr(mdof_model,param)[j]*alphas[param[:-1]])\n",
    "                wri_str += '[{:.4f}]       '.format(true_params[param][j])\n",
    "            wri_str + '\\n'\n",
    "        \n",
    "        tqdma.write(write_string)\n",
    "    \n",
    "    epoch += 1\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'config' : pinn_config,\n",
    "    'data' : data,\n",
    "    'prediction' : prediction,\n",
    "    'ground_truth' : ground_truth,\n",
    "    'alphas' : alphas,\n",
    "    'epoch' : i,\n",
    "    'model' : mdof_model.state_dict(),\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "    'loss' : loss_hist,\n",
    "    'true_params' : true_params,\n",
    "}\n",
    "# torch.save(checkpoint,'checkpoints/mdof_stoch_instance_batch.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
