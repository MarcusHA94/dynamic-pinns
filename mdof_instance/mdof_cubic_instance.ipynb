{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDOF PINN - instance prediction\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "The example problem we solve here is the 3DOF nonlinear-stiffness oscillator defined in state space:\n",
    "$$\n",
    "\\dot{\\mathbf{z}} = \\mathbf{A}\\mathbf{z} + \\mathbf{A}_n\\mathbf{z}_n + \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "\\mathbf{z} = \\left\\{ x_1, x_2, ... , x_n, \\dot{x}_1, \\dot{x}_2, ... , \\dot{x}_n \\right\\}^T, \\quad\n",
    "\\mathbf{f} = \\left\\{ f_1, f_2, ... , f_n \\right\\}^T\n",
    "$$\n",
    "and $\\mathbf{z}_n$ is the nonlinear state vector.\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 0 & \\mathbf{I} \\\\ -\\mathbf{M}^{-1}\\mathbf{K} & -\\mathbf{M}^{-1}\\mathbf{C} \\end{bmatrix}, \\quad\n",
    "\\mathbf{A}_n = \\begin{bmatrix} 0 \\\\ -\\mathbf{M}^{-1} \\mathbf{K}_n \\end{bmatrix}, \\quad\n",
    "\\mathbf{H} = \\begin{bmatrix} 0 \\\\ \\mathbf{M}^{-1} \\end{bmatrix}\n",
    "$$\n",
    "with the initial conditions\n",
    "$$\n",
    "\\mathbf{x}(0) = \\mathbf{x}_0~~,~~\\dot{\\mathbf{x}}(0) = \\dot{\\mathbf{x}}_0\n",
    "$$\n",
    "\n",
    "As an example, for a 3DOF system with cubic nonlinearities, fixed at the first degree of freedom:\n",
    "$$\n",
    "\\mathbf{z}_n = g_n(\\mathbf{z}) = \\left\\{ x_1^3, (x_2-x_1)^3, (x_3-x_2)^3 \\right\\}^T, \\quad\n",
    "\\mathbf{K}_n = \\begin{bmatrix} k_{n,1} & -k_{n,2} & 0 \\\\ 0 & k_{n,2} & -k_{n,3} \\\\ 0 & 0 & k_{n,3} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mdof_pinn import mdof_pinn, normalise, ParamClipper\n",
    "import dynasim\n",
    "from mdof_solutions import gen_ndof_cantilever, add_noise\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.stats import qmc\n",
    "\n",
    "import string\n",
    "import findiff\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 2048\n",
    "time = np.linspace(0,120,nt)\n",
    "\n",
    "F0 = 1.0  # N\n",
    "n_dof = 3\n",
    "\n",
    "# set physical parameters\n",
    "k1 = 10.0\n",
    "c1 = 0.25\n",
    "m1 = 1.0\n",
    "kn_ = np.zeros((n_dof))\n",
    "kn_[0] = 20.0\n",
    "cn_ = np.zeros((n_dof))\n",
    "\n",
    "# create nonlinearity\n",
    "cubic_nonlin = dynasim.nonlinearities.exponent_stiffness(kn_, exponent=3, dofs=n_dof)\n",
    "\n",
    "# instantiate system\n",
    "system = dynasim.systems.cantilever(m1, c1, k1, dofs=n_dof, nonlinearity=cubic_nonlin)\n",
    "\n",
    "true_params = {\n",
    "    'm_' : system.m_,\n",
    "    'c_' : system.c_,\n",
    "    'k_' : system.k_,\n",
    "    'kn_' : kn_\n",
    "}\n",
    "\n",
    "# generate excitations\n",
    "system.excitations = [\n",
    "    dynasim.actuators.rand_phase_ms(\n",
    "        freqs = np.array([0.7, 0.85, 1.6, 1.8]),\n",
    "        Sx = np.ones(4)\n",
    "    ), None, None]\n",
    "\n",
    "x0 = np.array([-2.0, 0.0, 3.0])\n",
    "v0 = np.array([-2.0, 0.0, 0.0])\n",
    "z0 = np.concatenate((x0, v0), axis=0)\n",
    "\n",
    "data = system.simulate(time, z0=None)\n",
    "\n",
    "xx, vv = torch.tensor(data['x'], dtype=torch.float32).T, torch.tensor(data['xdot'], dtype=torch.float32).T\n",
    "F = system.f\n",
    "xx_noisy, vv_noisy = add_noise(xx, db=-10, seed=43810), add_noise(vv, db=-6, seed=13927)\n",
    "# aa = torch.zeros_like(xx)\n",
    "dt = time[1]-time[0]\n",
    "d_dt = findiff.FinDiff(0,dt)\n",
    "aa = torch.tensor(d_dt(vv.numpy()))\n",
    "state_gt = torch.cat((xx,vv),dim=1).T\n",
    "state_gt_dot = torch.cat((vv,aa),dim=1).T\n",
    "q_ = (state_gt[:n_dof] - torch.cat((torch.zeros(1,state_gt.shape[1]),state_gt[:n_dof-1])))**3\n",
    "time = torch.tensor(time).view(-1,1).to(torch.float32)\n",
    "F = torch.tensor(F.T).to(torch.float32)\n",
    "F_noisy = add_noise(F, db=-10, seed=1234)\n",
    "\n",
    "# slice out a number of points\n",
    "# sampler = qmc.Sobol(d=1, seed=43810)\n",
    "# sub_ind = np.sort(sampler.integers(l_bounds=nt, n=int(nt/4)), axis=0).squeeze()\n",
    "sub_ind = np.arange(0, int(nt/1), 1)\n",
    "\n",
    "t_data = time[sub_ind]\n",
    "x_data = xx_noisy[sub_ind,:]\n",
    "v_data = vv_noisy[sub_ind,:]\n",
    "F_data = F_noisy[sub_ind,:]\n",
    "\n",
    "if n_dof > 4:\n",
    "    sub_rows = n_dof // 4 + int((n_dof%4)!=0)\n",
    "    sub_cols = 4\n",
    "else:\n",
    "    sub_rows = 1\n",
    "    sub_cols = n_dof\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(4*sub_cols,8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(time, xx[:,p_count], color=\"tab:blue\", label=\"Displacement\")\n",
    "        axs[j*3,i].grid()\n",
    "        axs[j*3,i].scatter(t_data, x_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3,i].legend()\n",
    "\n",
    "        axs[j*3+1,i].plot(time, vv[:,p_count], color=\"tab:red\", label=\"Velocity\")\n",
    "        # axs[j*3+1,i].set_ylim((-10,10))\n",
    "        axs[j*3+1,i].grid()\n",
    "        axs[j*3+1,i].scatter(t_data, v_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+1,i].legend()\n",
    "\n",
    "        axs[j*3+2,i].plot(time, F[:,p_count], color=\"tab:gray\", label=\"Forcing\")\n",
    "        # axs[j*3+2,i].plot(time, aa[:,p_count], color=\"tab:gray\", label=\"Acceleration\")\n",
    "        # axs[j*3+2,i].scatter(t_data, F_data[:,p_count], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+2,i].legend()\n",
    "        \n",
    "        p_count += 1\n",
    "\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise and create some plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hat, alpha_t = normalise(time, \"range\")\n",
    "x_hat_gt, alpha_x = normalise(xx_noisy, \"range\", \"all\")\n",
    "v_hat_gt, alpha_v = normalise(vv_noisy, \"range\", \"all\")\n",
    "F_hat_gt, alpha_F = normalise(F_noisy, \"range\", \"all\")\n",
    "\n",
    "t_data = t_hat[sub_ind]\n",
    "x_data = x_hat_gt[sub_ind,:]\n",
    "v_data = v_hat_gt[sub_ind,:]\n",
    "F_data = F_hat_gt[sub_ind,:]\n",
    "\n",
    "T_hat = t_data[1].item()-t_data[0].item()\n",
    "\n",
    "fig, axs = plt.subplots(3*sub_rows,sub_cols,figsize=(4*sub_cols,8*sub_rows))\n",
    "p_count = 0\n",
    "for j in range(sub_rows):\n",
    "    for i in range(sub_cols):\n",
    "        axs[j*3,i].plot(t_hat, x_hat_gt[:,i], color=\"tab:blue\", label=\"Displacement\")\n",
    "        # axs[j*3,i].scatter(t_data, x_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3,i].grid()\n",
    "        axs[j*3,i].legend()\n",
    "\n",
    "        axs[j*3+1,i].plot(t_hat, v_hat_gt[:,i], color=\"tab:red\", label=\"Velocity\")\n",
    "        # axs[j*3+1,i].scatter(t_data, v_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+1,i].grid()\n",
    "        axs[j*3+1,i].legend()\n",
    "\n",
    "        axs[j*3+2,i].plot(t_hat, F_hat_gt[:,i], color=\"tab:gray\", label=\"Forcing\")\n",
    "        # axs[j*3+2,i].scatter(t_data, F_data[:,i], color=\"tab:orange\", label=\"Training data\")\n",
    "        axs[j*3+2,i].legend()\n",
    "\n",
    "        p_count += 1\n",
    "        if p_count == n_dof:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_uppercase)\n",
    "mosaic_key = ''\n",
    "alph_count = 0\n",
    "for j in range(sub_rows):\n",
    "    mosaic_key += ''.join(alphabet[alph_count:alph_count+sub_cols]) + ';' + ''.join(alphabet[alph_count+sub_cols:alph_count+2*sub_cols]) + ';'\n",
    "    alph_count += 2*sub_cols\n",
    "mosaic_key += ''.join([alphabet[alph_count]]*sub_cols)\n",
    "print(mosaic_key)\n",
    "\n",
    "def plot_joint_loss_hist(ax,loss_hist):\n",
    "    n_epoch = len(loss_hist)\n",
    "    labels = [\"L_obs\", \"L_ic\", \"L_cc\", \"L_ode\", \"L\"]\n",
    "    colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"tab:cyan\", \"black\"]\n",
    "    # labels = [\"L_obs\", \"L_ic\", \"L_ode\", \"L_ed_b\", \"L_ed_p\", \"L\"]\n",
    "    # colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"black\"]\n",
    "    ax.cla()\n",
    "    for i in range(len(labels)):\n",
    "        ax.plot(np.arange(1,n_epoch+1),loss_hist[:,i],color=colors[i],label=labels[i])\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_result(axs_m, ground_truth, data, prediction, alphas, R_=None, axs2=None):\n",
    "    for ax in axs_m:\n",
    "        axs_m[ax].cla()\n",
    "        if axs2 is not None:\n",
    "            axs2[ax].cla()\n",
    "    # axs = np.array([[axs_m[alphabet[i]] for i in range(n_dof)],[axs_m[alphabet[n_dof+i]] for i in range(n_dof)]])\n",
    "    axs_top_list = []\n",
    "    for j in range(sub_rows):\n",
    "        axs_top_list.append([axs_m[alphabet[2*sub_cols*j+i]] for i in range(sub_cols)])\n",
    "        axs_top_list.append([axs_m[alphabet[2*sub_cols*j+sub_cols+i]] for i in range(sub_cols)])\n",
    "    axs_top = np.array(axs_top_list)\n",
    "\n",
    "    if axs2 is not None:\n",
    "        axs2_top_list = []\n",
    "        for j in range(sub_rows):\n",
    "            axs2_top_list.append([axs2[alphabet[2*sub_cols*j+i]] for i in range(sub_cols)])\n",
    "            axs2_top_list.append([axs2[alphabet[2*sub_cols*j+sub_cols+i]] for i in range(sub_cols)])\n",
    "        axs2_top = np.array(axs2_top_list)\n",
    "\n",
    "    plot_keys = [\"x_hat\", \"v_hat\"]\n",
    "    plot_cols = [\"tab:blue\", \"tab:red\"]\n",
    "    p_count = 0\n",
    "    for j in range(sub_rows):\n",
    "        for i in range(sub_cols):\n",
    "            for n in range(2):\n",
    "                axs_top[j*2+n,i].plot(data[\"t_hat\"].detach()*alphas[\"t\"], data[plot_keys[n]][:,p_count].detach()*alphas[\"x\"], color=\"tab:olive\", linewidth=1, alpha=0.8, label='Training data')\n",
    "                axs_top[j*2+n,i].plot(ground_truth[\"t_hat\"]*alphas[\"t\"], ground_truth[plot_keys[n]][:,p_count]*alphas[\"x\"], color=\"grey\", linewidth=2, alpha=0.5, label=\"Exact solution\")\n",
    "                axs_top[j*2+n,i].plot(prediction[\"t_hat\"].detach()*alphas[\"t\"], prediction[plot_keys[n]][:,p_count]*alphas[\"x\"], color=plot_cols[n], linewidth=2, alpha=0.8, linestyle='--', label=\"Neural network prediction\")\n",
    "                # axs_top[j*2+n,i].scatter(data[\"t_hat\"].detach()*alphas[\"t\"], data[plot_keys[n]][:,p_count].detach()*alphas[\"x\"], s=30, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "                # l = axs[i,j].legend(frameon=False, fontsize=\"large\")\n",
    "                xL = torch.amax(ground_truth[\"t_hat\"])*alphas[\"t\"]\n",
    "                yL = torch.amax(torch.abs(ground_truth[plot_keys[n]][:,p_count]))*alphas[\"x\"]\n",
    "                axs_top[j*2+n,i].set_xlim(-0.05*xL, 1.05*xL)\n",
    "                axs_top[j*2+n,i].set_ylim(-1.1*yL, 1.1*yL)\n",
    "            if R_ is not None:\n",
    "                axs2_top[j*2,i].plot(prediction[\"t_hat\"].detach()*alphas[\"t\"], R_[:,p_count].detach(), color=\"tab:green\", linewidth=1, alpha=0.8, label=\"SS-Phys residual\")\n",
    "\n",
    "            p_count += 1\n",
    "            if p_count == n_dof:\n",
    "                break\n",
    "\n",
    "ground_truth = {\n",
    "    \"t_hat\" : t_hat.detach(),\n",
    "    \"x_hat\" : x_hat_gt.detach(),\n",
    "    \"v_hat\" : v_hat_gt.detach(),\n",
    "    \"F_hat\" : F_hat_gt\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"t_hat\" : t_data,\n",
    "    \"x_hat\" : x_data,\n",
    "    \"v_hat\" : v_data,\n",
    "    \"F_hat\" : F_data,\n",
    "    \"sub_ind\" : sub_ind\n",
    "}\n",
    "\n",
    "prediction = {\n",
    "    \"t_hat\" : t_hat,\n",
    "    \"x_hat\" : None,\n",
    "    \"v_hat\" : None,\n",
    "    \"F_hat\" : F_hat_gt\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n",
    "\n",
    "Neural network predicts over the full time domain:\n",
    "$$\n",
    "\\mathcal{N}_{\\mathbf{z}}(t), \\qquad \n",
    "\\mathbf{R} = \\partial_t \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A} \\mathcal{N}_{\\mathbf{z}} - \\mathbf{A}_n \\mathcal{N}_{\\mathbf{z}_n} - \\mathbf{H}\\mathbf{f}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(t;\\mathbf{\\theta}) := \\mathcal{L}_{obs} + \\mathcal{L}_{ic} + \\Lambda\\mathcal{L}_{ode}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{obs} = \\langle \\hat{\\mathbf{z}}^* - \\mathcal{N}_{\\hat{\\mathbf{z}}} \\rangle _{\\Omega_o}\n",
    "% \\mathcal{L}_{cc} = \\sum_{j=1}^{N_{d}} \\left\\langle \\alpha_{\\dot{x}}\\mathcal{N}_{\\hat{\\dot{x}}_j} - \\frac{\\alpha_x}{\\alpha_t}\\partial_{\\hat{t}} \\mathcal{N}_{\\hat{x}_j} \\right\\rangle _{\\Omega_d}\n",
    "$$\n",
    "<!-- $$\n",
    "\\mathcal{L}_{ic} = \\sum_{j=1}^{N_{d}}\\left[ \n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\frac{\\alpha_x}{\\alpha_t}\\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{x}\\hat{x}_{j,0} - \\alpha_x\\mathcal{N}_{\\hat{x}_j} \\right\\rangle ~~ + ~~\n",
    "\\left\\langle \\alpha_{\\dot{x}}\\hat{\\dot{x}}_{j,0} - \\alpha_{\\dot{x}}\\mathcal{N}_{\\hat{\\dot{x}}_j} \\right\\rangle\n",
    "\\right] _{\\Omega\\in\\{t=0\\}}\n",
    "$$ -->\n",
    "$$\n",
    "\\mathcal{L}_{cc} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[j,:] \\right\\rangle _{\\Omega_p}, \\qquad\n",
    "\\mathcal{L}_{ode} = \\sum_{j=1}^{N_{d}} \\left\\langle \\mathbf{R}[N_d+j,:] \\right\\rangle _{\\Omega_p}\n",
    "$$\n",
    "where,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{z};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^2_{*}\\bullet = \\frac{\\partial^2\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{t\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{c}\\frac{1}{\\alpha_t}\\partial_{\\hat{t}}\\hat{x} + \n",
    "\\tilde{k} \\hat{x} - \n",
    "\\frac{\\alpha_F}{\\alpha_x} \\hat{F} = 0 \n",
    "\\quad \\rightarrow \\quad \n",
    "\\hat{m} \\partial^2_{\\hat{t}}\\hat{x} + \n",
    "\\hat{c} \\partial_{\\hat{t}}\\hat{x} + \n",
    "\\hat{k}\\hat{x} - \\eta\\hat{F} = 0\n",
    "$$\n",
    "To scale loss function in a physically meaningful way, multiply the loss function by any of the following:\n",
    "$$\n",
    "\\Lambda = 1, \\alpha_t, \\alpha_t^2, \\alpha_x^, \\alpha_F^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_c = 1.0\n",
    "alpha_k = 1.0\n",
    "alpha_kn = 1.0\n",
    "alphas = {\n",
    "    \"c\" : alpha_c,\n",
    "    \"k\" : alpha_k,\n",
    "    \"kn\" : alpha_kn,\n",
    "    \"t\" : alpha_t,\n",
    "    \"x\" : alpha_x,\n",
    "    \"v\" : alpha_v,\n",
    "    \"F\" : alpha_F\n",
    "}\n",
    "\n",
    "nct = nt  # number of collocation points\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "pinn_config = {\n",
    "    \"n_input\" : 1,\n",
    "    \"n_output\" : 2*n_dof,\n",
    "    \"n_hidden\" : 16,\n",
    "    \"n_layers\" : 4,\n",
    "    \"n_dof\" : n_dof,\n",
    "    \"nct\" : nct,\n",
    "    \"nonlinearity\" : \"cubic\",\n",
    "    \"phys_params\" : {\n",
    "        \"m_\" : {\n",
    "            \"type\" : \"constant\",\n",
    "            \"value\" : torch.tensor(system.m_, dtype=torch.float32)\n",
    "        },\n",
    "        \"c_\" : {\n",
    "            \"type\" : \"variable\",\n",
    "            \"value\" : torch.tensor(system.c_, dtype=torch.float32)\n",
    "        },\n",
    "        \"k_\" : {\n",
    "            \"type\" : \"variable\",\n",
    "            \"value\" : torch.tensor(system.k_, dtype=torch.float32)\n",
    "        },\n",
    "        \"kn_\" : {\n",
    "            \"type\" : \"variable\",\n",
    "            \"value\" : torch.tensor(kn_, dtype=torch.float32)\n",
    "        },\n",
    "    },\n",
    "    \"param_func\" : gen_ndof_cantilever,\n",
    "    \"alphas\" : alphas,\n",
    "    \"ode_norm_Lambda\" : alpha_x.item(),\n",
    "    \"forcing\" : F\n",
    "}\n",
    "\n",
    "# configure PINN\n",
    "mdof_model = mdof_pinn(pinn_config)\n",
    "mdof_model.set_colls_and_obs(data, prediction)\n",
    "\n",
    "# configure optimiser\n",
    "learning_rate = 1e-2\n",
    "betas = (0.99,0.999)\n",
    "optimizer = torch.optim.Adam(mdof_model.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "clipper = ParamClipper()\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    mosaic_key,\n",
    "    figsize=(18,16),\n",
    "    facecolor='w'\n",
    ")\n",
    "axs2 = axs.copy()\n",
    "for key, ax in axs.items():\n",
    "    axs2[key] = ax.twinx()\n",
    "\n",
    "print_step = 2000\n",
    "loss_hist=[]\n",
    "lambds = {\n",
    "    'obs' : 100.0,\n",
    "    'ic' : 0.0,\n",
    "    'ode' : 00.0,\n",
    "    'cc' : 0.0\n",
    "}\n",
    "\n",
    "mdof_model.set_switches(lambds)\n",
    "\n",
    "epochs = int(1e6)\n",
    "for i in tqdm(range(epochs)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses, residuals = mdof_model.loss_func(lambds)\n",
    "    loss_hist.append([loss_it.item() for loss_it in losses] + [loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    mdof_model.apply(clipper)\n",
    "\n",
    "    if (i+1) % print_step == 0:\n",
    "\n",
    "        z_pred = mdof_model.predict()\n",
    "\n",
    "        prediction[\"x_hat\"] = z_pred.detach()[:,:n_dof]\n",
    "        prediction[\"v_hat\"] = z_pred.detach()[:,n_dof:]\n",
    "\n",
    "        plot_result(axs, ground_truth, data, prediction, alphas)\n",
    "\n",
    "        plot_joint_loss_hist(axs[alphabet[2*n_dof]], np.array(loss_hist))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "        tqdma.write(\"c :                       k :                     kn : \")\n",
    "        for j in range(n_dof):\n",
    "            wri_str = '%d : ' % (j+1)\n",
    "            for param in ['c_','k_','kn_']:\n",
    "                if pinn_config['phys_params'][param]['type']=='constant':\n",
    "                    wri_str += '%.4f ' % getattr(mdof_model,param)[j]\n",
    "                elif pinn_config['phys_params'][param]['type']=='variable':\n",
    "                    wri_str += '%.4f ' % (getattr(mdof_model,param)[j]*alphas[param[:-1]])\n",
    "                wri_str += '[%.4f]       ' % true_params[param][j]\n",
    "            tqdma.write(wri_str)\n",
    "\n",
    "display.clear_output()\n",
    "tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "tqdma.write(\"c :                       k :                     kn : \")\n",
    "for j in range(n_dof):\n",
    "    wri_str = '%d : ' % (j+1)\n",
    "    for param in ['c_','k_','kn_']:\n",
    "        if pinn_config['phys_params'][param]['type']=='constant':\n",
    "            wri_str += '%.4f ' % getattr(mdof_model,param)[j]\n",
    "        elif pinn_config['phys_params'][param]['type']=='variable':\n",
    "            wri_str += '%.4f ' % (getattr(mdof_model,param)[j]*alphas[param[:-1]])\n",
    "        wri_str += '[%.4f]       ' % true_params[param][j]\n",
    "    tqdma.write(wri_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch' : i,\n",
    "    'model' : mdof_model.state_dict(),\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "    'loss' : loss_hist,\n",
    "    # 'scheduler' : scheduler\n",
    "}\n",
    "torch.save(checkpoint,'checkpoints/mdof_cubic_instance.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/mdof_cubic_instance.pth')\n",
    "loss_hist = checkpoint[\"loss\"]\n",
    "mdof_model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for name, param in mdof_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "mdof_model.c_.requires_grad = True\n",
    "mdof_model.k_.requires_grad = True\n",
    "mdof_model.kn_.requires_grad = True\n",
    "\n",
    "clipper = ParamClipper()\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 5e-3\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    mosaic_key,\n",
    "    figsize=(18,16),\n",
    "    facecolor='w'\n",
    ")\n",
    "axs2 = axs.copy()\n",
    "for key, ax in axs.items():\n",
    "    axs2[key] = ax.twinx()\n",
    "\n",
    "print_step = 1000\n",
    "lambds = {\n",
    "    'obs' : 100.0,\n",
    "    'ic' : 0.0,\n",
    "    'ode' : 100.0,\n",
    "    'cc' : 10.0\n",
    "}\n",
    "\n",
    "mdof_model.set_switches(lambds)\n",
    "\n",
    "epochs = int(5e5)\n",
    "for i in tqdm(range(epochs)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses, residuals = mdof_model.loss_func(lambds)\n",
    "    loss_hist.append([loss_it.item() for loss_it in losses] + [loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    mdof_model.apply(clipper)\n",
    "\n",
    "    # mdof_model.apply(clipper)\n",
    "\n",
    "    if (i+1) % print_step == 0:\n",
    "\n",
    "        X_pred = mdof_model.predict()\n",
    "\n",
    "        prediction[\"x_hat\"] = X_pred.detach()[:,:n_dof]\n",
    "        prediction[\"v_hat\"] = X_pred.detach()[:,n_dof:]\n",
    "\n",
    "        plot_result(axs, ground_truth, data, prediction, alphas)#, residuals['R_ode'], axs2)\n",
    "\n",
    "        plot_joint_loss_hist(axs[alphabet[2*n_dof]], np.array(loss_hist))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "        tqdma.write(\"c :                       k :                     kn : \")\n",
    "        for j in range(n_dof):\n",
    "            wri_str = '%d : ' % (j+1)\n",
    "            for param in ['c_','k_','kn_']:\n",
    "                if pinn_config['phys_params'][param]['type']=='constant':\n",
    "                    wri_str += '%.4f ' % getattr(mdof_model,param)[j]\n",
    "                elif pinn_config['phys_params'][param]['type']=='variable':\n",
    "                    wri_str += '%.4f ' % (getattr(mdof_model,param)[j]*alphas[param[:-1]])\n",
    "                wri_str += '[%.4f]       ' % true_params[param][j]\n",
    "            tqdma.write(wri_str)\n",
    "\n",
    "display.clear_output()\n",
    "tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss_hist[i][-1]))\n",
    "tqdma.write(\"c :                       k :                     kn : \")\n",
    "for j in range(n_dof):\n",
    "    wri_str = '%d : ' % (j+1)\n",
    "    for param in ['c_','k_','kn_']:\n",
    "        if pinn_config['phys_params'][param]['type']=='constant':\n",
    "            wri_str += '%.4f ' % getattr(mdof_model,param)[j]\n",
    "        elif pinn_config['phys_params'][param]['type']=='variable':\n",
    "            wri_str += '%.4f ' % (getattr(mdof_model,param)[j]*alphas[param[:-1]])\n",
    "        wri_str += '[%.4f]       ' % true_params[param][j]\n",
    "    tqdma.write(wri_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = {\n",
    "    'epoch' : i,\n",
    "    # 'gt_config' : config,\n",
    "    'osa_config' : pinn_config,\n",
    "    'model' : pinn_config.state_dict(),\n",
    "    'loss' : loss_hist,\n",
    "    'ground_truth' : ground_truth,\n",
    "    'data' : data,\n",
    "    'prediction' : prediction,\n",
    "    'alphas' : alphas\n",
    "}\n",
    "\n",
    "torch.save(result_data, 'results/osa_forced_mdof.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
