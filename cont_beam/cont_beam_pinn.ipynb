{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cantilever beam physics-informed neural network (PINN)\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "The example problem we solve here is a continuous beam:\n",
    "$$ \\rho A \\frac{\\partial^2w}{\\partial t^2} + E I \\frac{\\partial^4w}{\\partial x^4} + \\rho A c \\frac{\\partial w}{\\partial t} = f(t) $$\n",
    "$$ w(x, t) = \\sum_{j=1}^{\\infty}\\varphi_j(x)q_j(t) \\approx \\sum_{j=1}^{n}\\varphi_j(x)q_j(t) $$\n",
    "where $\\varphi_j$ and $q_j$ are the $j^{th}$ modal shape and coordinate of the $j^{th}$ mode, respectively.\n",
    "$$\n",
    "\\mathbf{M}\\ddot{\\mathbf{q}}(t) + \\mathbf{C}\\dot{\\mathbf{q}}(t) + \\mathbf{K}\\mathbf{q}(t) = \\mathbf{S_p}\\mathbf{p}(t)\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "\\mathbf{M} = \\rho A \\int_0^l\\mathbf{\\psi}(x)\\mathbf{\\psi}^T(x) dx, \\qquad\n",
    "\\mathbf{C} = \\rho Ac \\int_0^l\\mathbf{\\psi}(x)\\mathbf{\\psi}^T(x) dx, \\qquad\n",
    "\\mathbf{K} = EI \\int_0^l\\mathbf{\\psi}(x){\\mathbf{\\psi}''''}^T(x) dx\n",
    "$$\n",
    "in the state space,\n",
    "$$\n",
    "\\dot{\\mathbf{\\tau}}(t) = \\mathbf{A} \\mathbf{\\tau}(t) + \\mathbf{H} \\mathbf{f}(t)\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "\\mathbf{\\tau} = \\begin{bmatrix} \\mathbf{q}(t) \\\\ \\dot{\\mathbf{q}}(t) \\end{bmatrix}, \\qquad\n",
    "\\mathbf{A} = \\begin{bmatrix} 0 & \\mathbf{I} \\\\ -\\mathbf{M}^{-1}\\mathbf{K} & -\\mathbf{M}^{-1}\\mathbf{C} \\end{bmatrix}, \\qquad\n",
    "\\mathbf{H} = \\begin{bmatrix} 0 \\\\ \\mathbf{M}^{-1} \\end{bmatrix}, \\qquad\n",
    "\\mathbf{f}(t) = m_e g\\mathbf{\\Psi}(x_e)\\mathbf{I}\n",
    "$$\n",
    "And the measurement vector is,\n",
    "$$ \\mathbf{y}(t) = \\begin{bmatrix} S_d & 0 \\\\ 0 & S_a \\end{bmatrix} \\begin{bmatrix} \\mathbf{q}(t) \\\\ \\ddot{\\mathbf{q}}(t) \\end{bmatrix} $$\n",
    "\n",
    "<!-- $$\n",
    "\\mathrm{argmin}\\mathcal{L}(\\mathbf{x},t;\\mathbf{\\theta}) := \\mathcal{L}_a + \\lambda\\left[ \\mathcal{L}_{ode} \\right]\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_a = \\langle \\varphi_{kj}\\hat{q}_j - y_k^* \\rangle\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_{ode,j} = \\langle \\hat{\\ddot{q}}_j + 2\\hat{\\zeta}\\hat{\\omega}_j\\hat{\\dot{q}}_j + \\hat{\\omega}^2\\hat{q}_j - p^*_j \\rangle, \\qquad\n",
    "\\mathcal{L}_{ode,j} = \\langle \\hat{m} \\partial^2_{\\hat{t}}\\mathcal{N}_{\\hat{q}} + \\hat{c} \\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}} + \\hat{k}\\mathcal{N}_{\\hat{x}} \\rangle _{\\Omega_d}\n",
    "$$\n",
    "where,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{x};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^2_{*}\\bullet = \\frac{\\partial^2\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{x\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{q} + \\tilde{c}\\frac{1}{\\alpha_t}\\partial_{\\hat{t}}\\hat{q} + \\tilde{k} \\hat{q} = 0, \\qquad \\hat{m} \\partial^2_{\\hat{t}}\\hat{q} + \\hat{c} \\partial_{\\hat{t}}\\hat{q} + \\hat{k}\\hat{q} = 0\n",
    "$$\n",
    "> trad(itional)\n",
    "$$\n",
    "\\hat{m} = \\frac{1}{\\alpha_t^2}, \\quad \\hat{c} = \\tilde{c}\\frac{1}{\\alpha_t}, \\quad \\hat{k} = \\tilde{k}\n",
    "$$\n",
    "> up_time\n",
    "$$\n",
    "\\hat{m} = \\frac{1}{\\alpha_t}, \\quad \\hat{c} = \\tilde{c}, \\quad \\hat{k} = \\tilde{k}\\alpha_t\n",
    "$$\n",
    "> up_time2\n",
    "$$\n",
    "\\hat{m} = 1, \\quad \\hat{c} = \\tilde{c}\\alpha_t, \\quad \\hat{k} = \\tilde{k}\\alpha_t^2\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cont_pinn import bbnn, beam_pinn, normalise\n",
    "from beam_solutions import cont_beam\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_beam_params(E, rho, EI, pA):\n",
    "    I_ = EI/E\n",
    "    A_ = pA/rho\n",
    "    h = (2*I_/A_)**(0.5)\n",
    "    b = A_/h\n",
    "    I = (1/12) * b * h**3\n",
    "    A = b * h\n",
    "    return I, A, b, h\n",
    "\n",
    "def plot_vib_surface(ax, x, time, w, sub_samp=10):\n",
    "    if len(x.shape) == 1 and len(time.shape) == 1:\n",
    "        xx, tt = torch.meshgrid(x[::sub_samp], time[::sub_samp], indexing=\"ij\")\n",
    "        x_ = unroll_mat(xx)\n",
    "        time_ = unroll_mat(tt)\n",
    "    else:\n",
    "        if x.shape[1] > 1 and time.shape[1] > 1:\n",
    "            x_ = unroll_mat(x[::sub_samp,:][:,::sub_samp])\n",
    "            time_ = unroll_mat(time[::sub_samp,:][:,::sub_samp])\n",
    "        elif x.shape[1] <= 1 and time.shape[1] <= 1:\n",
    "            xx, tt = torch.meshgrid(x[::sub_samp], time[::sub_samp], indexing=\"ij\")\n",
    "            x_ = unroll_mat(xx)\n",
    "            time_ = unroll_mat(tt)\n",
    "    if len(w.shape) == 1:\n",
    "        w_ = w[::sub_samp]\n",
    "    else:\n",
    "        if w.shape[1] > 1:\n",
    "            w_ = unroll_mat(w[::sub_samp,:][:,::sub_samp])\n",
    "        else:\n",
    "            w_ = w[::sub_samp]\n",
    "\n",
    "    ax.plot_trisurf(x_, time_, w_, cmap=cm.plasma, linewidth=0.1, edgecolor='black')\n",
    "    ax.set_xlabel('x, m')\n",
    "    ax.set_ylabel('Time, s')\n",
    "    ax.set_zlabel('w, m')\n",
    "\n",
    "def plot_vib_scatter(ax, x, time, w, color='grey'):\n",
    "    ax.scatter3D(x, time, w, color=color)\n",
    "    ax.set_xlabel('x, m')\n",
    "    ax.set_ylabel('Time, s')\n",
    "    ax.set_zlabel('w, m')\n",
    "\n",
    "def unroll_mat(data):\n",
    "    nn = data.shape[0]*data.shape[1]\n",
    "    data_ = torch.zeros(nn)\n",
    "    di = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            data_[di] = data[i,j]\n",
    "            di += 1\n",
    "    return data_\n",
    "\n",
    "def roll_mat(data, n1, n2):\n",
    "    data_ = torch.zeros((n1, n2))\n",
    "    d_count = 0\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            data_[i, j] = data[d_count]\n",
    "            d_count += 1\n",
    "    return data_\n",
    "\n",
    "def gen_NN_data(x, t, w, phi=None):\n",
    "    nd = x.shape[0] * t.shape[0]\n",
    "    dc = 0\n",
    "    x_vec, t_vec, w_vec = np.zeros((nd,1)), np.zeros((nd,1)), np.zeros((nd,1))\n",
    "    if phi != None:\n",
    "        phi_vec = np.zeros((nd, phi.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(t.shape[0]):\n",
    "            x_vec[dc] = x[i]\n",
    "            t_vec[dc] = t[j]\n",
    "            w_vec[dc] = w[i,j]\n",
    "            if phi != None:\n",
    "                phi_vec[dc,:] = phi[i,:]\n",
    "            dc += 1\n",
    "    if phi == None:\n",
    "        return torch.tensor(x_vec, dtype=torch.float32).view(-1,1), torch.tensor(t_vec, dtype=torch.float32).view(-1,1), torch.tensor(w_vec, dtype=torch.float32).view(-1,1)\n",
    "    else:\n",
    "        return torch.tensor(x_vec, dtype=torch.float32).view(-1,1), torch.tensor(t_vec, dtype=torch.float32).view(-1,1), torch.tensor(w_vec, dtype=torch.float32).view(-1,1), torch.tensor(phi_vec, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the analytical solution over the full domain\n",
    "E = 1e7\n",
    "rho = 2700\n",
    "EI_ = 5e-3\n",
    "pA = 1.0\n",
    "c = 0.0\n",
    "l = 1.0\n",
    "w1 = (pi**2) * ((EI_)/(pA*(l**4)))**0.5\n",
    "f1 = w1/(2*pi)\n",
    "I, A, b, h = gen_beam_params(E, rho, EI_, pA)\n",
    "\n",
    "nx = 64\n",
    "nt = 64\n",
    "n_modes = 6\n",
    "\n",
    "beam_kwargs_sep = {\n",
    "    \"E\" : E,\n",
    "    \"I\" : I,\n",
    "    \"rho\" : rho,\n",
    "    \"A\" : A,\n",
    "    \"l\" : l\n",
    "}\n",
    "\n",
    "beam_kwargs_cmb = {\n",
    "    \"EI\" : EI_,\n",
    "    \"pA\" : pA,\n",
    "    \"l\" : l\n",
    "}\n",
    "\n",
    "# ss_beam = cont_beam(\"sep_vars\", beam_kwargs_sep)\n",
    "ss_beam = cont_beam(\"cmb_vars\", **beam_kwargs_cmb)\n",
    "ss_beam.gen_modes(\"ss-ss\", n_modes, nx)\n",
    "\n",
    "xx = ss_beam.xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 1\n",
    "init_disp = 100e-3\n",
    "v0 = 0.0\n",
    "vv0 = torch.zeros(nx)\n",
    "\n",
    "t = torch.linspace(0,2.5,nt)\n",
    "\n",
    "# init_load = {\n",
    "#     \"type\" : \"point_load\",\n",
    "#     \"f0\" : f0,\n",
    "#     \"load_coord\" : 0.1,\n",
    "# }\n",
    "# w0 = ss_beam.init_cond_load(init_load)\n",
    "# wxt, wxt_n = ss_beam.free_vibration(t, w0, vv0, 0.0)\n",
    "\n",
    "# forcing = {\n",
    "#     \"type\" : \"step_load\",\n",
    "#     \"force_mag\" : f0,\n",
    "#     \"load_coord\" : 0.1\n",
    "# }\n",
    "# forcing = {\n",
    "#     \"type\" : \"harmonic\",\n",
    "#     \"force_mag\" : f0,\n",
    "#     \"load_coord\" : 0.1,\n",
    "#     \"frequency\" : 0.8\n",
    "# }\n",
    "# wxt, wxt_n = ss_beam.forced_vibration(t, forcing)\n",
    "\n",
    "# init_disp_loc = 0.1\n",
    "# init_disp_id = torch.argmin(torch.abs(ss_beam.xx - init_disp_loc))\n",
    "# w0 = torch.zeros(nx)\n",
    "# w0[init_disp_id] = init_disp\n",
    "w0_func = lambda x : 0.01 * torch.sin(3*pi*x)\n",
    "w0 = w0_func(ss_beam.xx)\n",
    "wxt, wxtd, wxtdd, wxt_n = ss_beam.free_vibration(t, w0, vv0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sens = 10\n",
    "sens_distance = xx[-1]/(n_sens+1)  # distance between sensors\n",
    "s_locs = np.arange(sens_distance, xx[-1], sens_distance)  # target location of sensors\n",
    "s_ids = [np.argmin(np.abs(xx - s_locs[i])) for i in range(n_sens)]  # sensor ids from gt data\n",
    "# s_ids = s_ids[:-int(n_sens/2)]\n",
    "\n",
    "plot_locs = [0.3, 0.45, 0.80]  # target location for plotting\n",
    "plot_ids_ss = [np.argmin(np.abs(xx[np.array(s_ids)] - plot_locs[i])) for i in range(3)]  # plotting location ids, relative to sensor dataset\n",
    "plot_locs_ = [xx[s_ids[k]] for k in plot_ids_ss]\n",
    "plot_ids_gt = [np.argmin(np.abs(xx - plot_locs_[i])) for i in range(3)]  # plotting location ids, relative to gt dataset, by taking closest of sensor dataset\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize = (12,8))\n",
    "for i in range(3):\n",
    "    axs[i,0].plot(t, wxt[s_ids[i],:])\n",
    "    axs[i,1].plot(t, wxtd[s_ids[i],:])\n",
    "    axs[i,2].plot(t, wxtdd[s_ids[i],:])\n",
    "axs[0,0].set_title('Displacement')\n",
    "axs[0,1].set_title('Velocity')\n",
    "axs[0,2].set_title('Acceleration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, subplot_kw = {\"projection\":\"3d\"}, figsize=(12,12))\n",
    "axs = axs.ravel()\n",
    "plot_vib_surface(axs[0], xx, t, wxt, 1)\n",
    "axs[0].set_title('Displacement')\n",
    "plot_vib_surface(axs[1], xx, t, wxtd, 1)\n",
    "axs[1].set_title('Velocity')\n",
    "plot_vib_surface(axs[2], xx, t, wxtdd, 1)\n",
    "axs[2].set_title('Acceleration')\n",
    "plot_vib_surface(axs[3], xx, t, wxt_n[:,:,2], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hat, alpha_t = normalise(t, \"range\")\n",
    "x_hat, alpha_x = normalise(xx, \"range\")\n",
    "w_hat, alpha_w = normalise(wxt, \"range\", \"all\")\n",
    "wn_hat = wxt_n / alpha_w\n",
    "\n",
    "alphas = {\n",
    "    \"x\" : alpha_x,\n",
    "    \"t\" : alpha_t,\n",
    "    \"w\" : alpha_w\n",
    "}\n",
    "\n",
    "# sub_ind_x = np.arange(0, int(nx/4), 1)\n",
    "sub_ind_x = torch.tensor(s_ids)  # indices for sub-selection along x (sensor data)\n",
    "sub_ind_t = np.arange(0, int(nt/1), 4)  # indices for sub-selection along time\n",
    "\n",
    "x_data_vec = x_hat[sub_ind_x]\n",
    "t_data_vec = t_hat[sub_ind_t]\n",
    "phi_data_vec = ss_beam.phi_n[sub_ind_x, 1:]\n",
    "\n",
    "w_data_mat = w_hat[:, sub_ind_t][sub_ind_x, :]\n",
    "wn_data_mat = wn_hat[:, sub_ind_t, :][sub_ind_x, :, :]\n",
    "\n",
    "sens_ids_ = [torch.argmin(torch.abs(x_data_vec - plot_locs[i]/alpha_x)) for i in range(3)]\n",
    "# s_ids = [np.argmin(np.abs(xx - s_locs[i])) for i in range(3)]\n",
    "\n",
    "x_data, t_data, w_data, phi_data = gen_NN_data(x_data_vec, t_data_vec, w_data_mat, phi_data_vec)\n",
    "x_pred, t_pred, w_pred, phi_pred = gen_NN_data(x_hat, t_hat, w_hat, ss_beam.phi_n)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, subplot_kw = {\"projection\":\"3d\", \"proj_type\":\"ortho\"}, figsize=(12,12))\n",
    "plot_vib_surface(axs[0], x_hat, t_hat, w_hat, 1)\n",
    "# axs[0].view_init(elev=0, azim=-90)\n",
    "\n",
    "# plot_vib_surface(axs[1], x_data_vec, t_data_vec, w_data_mat, 1)\n",
    "plot_vib_scatter(axs[1], x_data, t_data, w_data)\n",
    "# axs[1].view_init(elev=0, azim=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_sens(axs_m, t_hat, w_hat, t_data, w_data, wpred, alphas):\n",
    "    axs = [axs_m[\"A\"], axs_m[\"B\"], axs_m[\"C\"]]\n",
    "    for ax in axs:\n",
    "        ax.cla()\n",
    "    xL = torch.amax(t_hat)*alphas[\"t\"]\n",
    "    for i in range(3):\n",
    "        axs[i].plot(t_hat * alphas[\"t\"], w_hat[s_ids[i],:] * alphas[\"w\"], color=\"grey\", linewidth=1, alpha=0.8, label=\"Exact solution\")\n",
    "        axs[i].plot(t_hat * alphas[\"t\"], wpred[s_ids[i],:] * alphas[\"w\"], color=\"tab:blue\", linewidth=1.5, alpha=0.8, label=\"Neural network prediction\")\n",
    "        axs[i].scatter(t_data * alphas[\"t\"], w_data[sens_ids_[i],:] * alphas[\"w\"], s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "        yL = torch.amax(torch.abs(w_hat[s_ids[i],:]))*alphas[\"w\"]\n",
    "        axs[i].set_xlim(-0.05*xL, 1.05*xL)\n",
    "        axs[i].set_ylim(-1.1*yL, 1.1*yL)\n",
    "\n",
    "def plot_nn_update(axs_m, t_hat, w_hat, t_data, w_data, xpred, tpred, wpred, alphas):\n",
    "    axs = [axs_m[\"A\"], axs_m[\"B\"], axs_m[\"C\"]]\n",
    "    for ax in axs:\n",
    "        ax.cla()\n",
    "    xL = torch.amax(t_hat)*alphas[\"t\"]\n",
    "    for i in range(3):\n",
    "        axs[i].plot(t_hat * alphas[\"t\"], w_hat[s_ids[i],:] * alphas[\"w\"], color=\"grey\", linewidth=1, alpha=0.8, label=\"Exact solution\")\n",
    "        axs[i].plot(tpred * alphas[\"t\"], wpred[s_ids[i],:] * alphas[\"w\"], color=\"tab:blue\", linewidth=1.5, alpha=0.8, label=\"Neural network prediction\")\n",
    "        axs[i].scatter(t_data * alphas[\"t\"], w_data[sens_ids_[i],:] * alphas[\"w\"], s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n",
    "        yL = torch.amax(torch.abs(w_hat[s_ids[i],:]))*alphas[\"w\"]\n",
    "        axs[i].set_xlim(-0.05*xL, 1.05*xL)\n",
    "        axs[i].set_ylim(-1.1*yL, 1.1*yL)\n",
    "    axs_m['E'].cla()\n",
    "    plot_vib_surface(axs_m['E'], xpred, tpred, wpred, 1)\n",
    "\n",
    "def plot_loss_hist(ax,loss_hist):\n",
    "    ax.cla()\n",
    "    n_epoch = len(loss_hist)\n",
    "    ax.plot(np.arange(1,n_epoch+1),loss_hist,'b')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "def plot_joint_loss_hist(ax, loss_hist):\n",
    "    n_epoch = len(loss_hist)\n",
    "    ax.cla()\n",
    "    loss_labs = ['Observation loss', 'Physics loss', 'BC loss', 'IC loss', 'Total loss']\n",
    "    loss_cols = ['b', 'r', 'g', 'm', 'k']\n",
    "    for i in range(loss_hist.shape[1]):\n",
    "        ax.plot(np.arange(1,n_epoch+1), loss_hist[:,i], loss_cols[i], label=loss_labs[i])\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Black-box' single-network architecture\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}\\mathcal{L}(\\mathbf{x},t;\\mathbf{\\theta}) := \\langle  \\hat{w}^* - \\mathcal{N}_{\\hat{w}} \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train standard neural network to fit training data\n",
    "torch.manual_seed(43810)\n",
    "epochs = int(5e3)\n",
    "bb_model = bbnn(2,1,32,6)\n",
    "optimizer = torch.optim.Adam(bb_model.parameters(),lr=1e-3)\n",
    "print_step = 50\n",
    "loss_hist = []\n",
    "\n",
    "# fig, axs = plt.subplots(1,2,figsize=(18,6),facecolor='w')\n",
    "# fig, axs = plt.subplot_mosaic(\"AD;BD;CD\", figsize=(18,12), facecolor='w')\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    \"AD;BE;CE\",\n",
    "    per_subplot_kw = {\n",
    "    \"E\" : {\"projection\" : \"3d\"}\n",
    "    },\n",
    "    figsize=(18,12), \n",
    "    facecolor='w'\n",
    "    )\n",
    "for i in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = bb_model.loss_func(x_data, t_data, w_data)\n",
    "    loss_hist.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % print_step == 0: \n",
    "        \n",
    "        wpred_vec = bb_model.predict(x_pred, t_pred).detach()\n",
    "        wpred = roll_mat(wpred_vec, nx, nt)\n",
    "        \n",
    "        plot_nn_update(axs, t_hat, w_hat, t_data_vec, w_data_mat, x_hat, t_hat, wpred, alphas)\n",
    "    \n",
    "        plot_loss_hist(axs[\"D\"],loss_hist)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "display.clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN\n",
    "Some shorthand definitions,\n",
    "$$ \\mathcal{N}_{\\bullet} = \\mathcal{N}_{\\bullet}(\\mathbf{x};\\mathbf{\\theta}), \\qquad \n",
    "\\partial_{*}\\bullet = \\frac{\\partial\\bullet}{\\partial *}, \\qquad \n",
    "\\partial^n_{*}\\bullet = \\frac{\\partial^n\\bullet}{\\partial *^2}, \\qquad\n",
    "\\langle\\bullet\\rangle _{\\Omega_{\\kappa}} = \\frac{1}{N_{\\kappa}}\\sum_{x\\in\\Omega_{\\kappa}}\\left|\\left|\\bullet\\right|\\right|^2 $$\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}\\mathcal{L}(\\mathbf{x},t;\\mathbf{\\theta}) := \\mathcal{L}_a + \\lambda_1 \\mathcal{L}_{ode} + \\lambda_2 \\mathcal{L}_{bc}\n",
    "$$\n",
    "From the boundary conditions:\n",
    "$$\n",
    "\\mathcal{L}_{bc} = \\langle \\mathcal{N}_w \\rangle _{\\Omega_{bc}}, \\qquad \\Omega_{bc} \\in x=\\{0.0,l\\}\n",
    "$$\n",
    "<!-- The paper forms the loss function using a SDOF model for each mode\n",
    "$$\n",
    "\\mathcal{L}_{ode,j} = \\langle \\hat{\\ddot{q}}_j + 2\\hat{\\zeta}\\hat{\\omega}_j\\hat{\\dot{q}}_j + \\hat{\\omega}^2\\hat{q}_j - p^*_j \\rangle, \\qquad\n",
    "\\mathcal{L}_{ode,j} = \\langle \\hat{m} \\partial^2_{\\hat{t}}\\mathcal{N}_{\\hat{q}} + \\hat{c} \\partial_{\\hat{t}}\\mathcal{N}_{\\hat{x}} + \\hat{k}\\mathcal{N}_{\\hat{x}} \\rangle _{\\Omega_d}\n",
    "$$\n",
    "Here, a suggested change is to use the strong form as the definition, -->\n",
    "$$\n",
    "\\mathcal{L}_{ode} = \\langle \\rho A \\partial_t^2 \\mathcal{N}_w + EI \\partial_x^4\\mathcal{N}_w \\rangle, \\qquad\n",
    "\\mathcal{L}_{ode} = \\langle \\hat{m} \\partial_t^2 \\mathcal{N}_w + \\hat{k} \\partial_x^4\\mathcal{N}_w \\rangle\n",
    "$$\n",
    "\n",
    "ODE loss function comes from including the normalisation of the parameters, then choosing the suitable range to aid optimisation.\n",
    "$$\n",
    "\\frac{\\rho A}{\\alpha_t^2} \\partial^2_{\\hat{t}}\\hat{w} + \\frac{EI}{\\alpha_x^4} \\partial_{\\hat{x}}^4 \\hat{w} = 0, \\quad \\rightarrow \\quad \n",
    "\\hat{m} \\partial^2_{\\hat{t}}\\hat{w} + \\hat{k}\\partial_{\\hat{x}}^4\\hat{w} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m} = \\frac{\\rho A}{\\alpha_t^2}, \\quad \\hat{k} = \\frac{EI}{\\alpha_x^4}\n",
    "$$\n",
    "\n",
    "To scale loss function in a physically meaningful way, multiply $\\hat{m}$, $\\hat{c}$, and $\\hat{k}$ by any (or combination) of the following:\n",
    "$$\n",
    "\\Lambda = \\alpha_t,~~\\alpha_t^2,~~\\alpha_x,~~\\alpha_x^2,~~\\alpha_x^3,~~\\alpha_x^4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_pA = pA*10\n",
    "alpha_EI = EI_*10\n",
    "alphas_p = [alpha_pA, alpha_EI]\n",
    "alphas_d = [alpha_t, alpha_x, alpha_w]\n",
    "alphas = {\n",
    "    \"pA\" : alpha_pA,\n",
    "    \"EI\" : alpha_EI,\n",
    "    \"t\" : alpha_t,\n",
    "    \"x\" : alpha_x,\n",
    "    \"w\" : alpha_w\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "beam_pinn_config = {\n",
    "    \"n_input\" : 2,\n",
    "    \"n_output\" : 1,\n",
    "    \"n_hidden\" : 32,\n",
    "    \"n_layers\" : 4,\n",
    "    \"nonlinearity\" : \"linear\",\n",
    "    \"phys_params\" : {\n",
    "        \"par_type\" : \"constant\",\n",
    "        \"pA\": pA,\n",
    "        \"EI\" : EI_\n",
    "    },\n",
    "    \"alphas\" : alphas,\n",
    "    \"ode_norm_Lambda\" : 1.0\n",
    "}\n",
    "\n",
    "# configure PINN\n",
    "beam_pi_model = beam_pinn(beam_pinn_config)\n",
    "\n",
    "x_pde = x_pred[:].requires_grad_()\n",
    "t_pde = t_pred[:].requires_grad_()\n",
    "x_data = x_data.requires_grad_()\n",
    "t_data = t_data.requires_grad_()\n",
    "w_data = w_data.requires_grad_()\n",
    "\n",
    "ic_ids = torch.unique(torch.cat((torch.where((t_pde==0.0))[0],torch.where((x_pde==1.0))[0]),dim=0))\n",
    "ic_vals = w0_func(x_pde[ic_ids]).requires_grad_()\n",
    "init_conds = {\n",
    "    'ids' : ic_ids,\n",
    "    'disp_vals' : ic_vals\n",
    "}\n",
    "\n",
    "bc_ids = torch.unique(torch.cat((torch.where((x_pde==0.0))[0],torch.where((x_pde==1.0))[0]),dim=0))\n",
    "bc_vals = torch.zeros(bc_ids.shape[0])\n",
    "bound_conds = {\n",
    "    'ids' : bc_ids,\n",
    "    'disp_vals' : bc_vals\n",
    "}\n",
    "\n",
    "beam_pi_model.set_conditions(init_conds, bound_conds)\n",
    "\n",
    "# configure optimiser\n",
    "epochs = int(400e3)\n",
    "learning_rate = 5e-3\n",
    "betas = (0.99, 0.999)\n",
    "print_step = 50\n",
    "optimizer = torch.optim.Adam(beam_pi_model.parameters(), lr=learning_rate, betas=betas)\n",
    "\n",
    "fig, axs = plt.subplot_mosaic(\n",
    "    \"AD;BE;CE\",\n",
    "    per_subplot_kw = {\n",
    "    \"E\" : {\"projection\" : \"3d\"}\n",
    "    },\n",
    "    figsize=(18,12), \n",
    "    facecolor='w'\n",
    "    )\n",
    "loss_hist = []\n",
    "\n",
    "lambds = {\n",
    "    'obs' : torch.tensor(1.0),\n",
    "    'pde' : torch.tensor(1e-8),\n",
    "    'bc1' : torch.tensor(1.0),\n",
    "    'bc2' : torch.tensor(1.0)\n",
    "}\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, losses = beam_pi_model.loss_func(x_pde, t_pde, x_data, t_data, w_data, lambds)\n",
    "    loss_hist.append([loss_it.item() for loss_it in losses] + [loss.item()])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % print_step == 0: \n",
    "        \n",
    "        wpred_vec = bb_model.predict(x_pred, t_pred).detach()\n",
    "        wpred = roll_mat(wpred_vec, nx, nt)\n",
    "        \n",
    "        plot_nn_update(axs, t_hat, w_hat, t_data_vec, w_data_mat, x_hat, t_hat, wpred, alphas)\n",
    "    \n",
    "        plot_joint_loss_hist(axs[\"D\"],loss_hist)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss))\n",
    "\n",
    "display.clear_output()\n",
    "tqdma.write(\"Epoch : %d ---- Loss: %.2e\" % (i+1,loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
